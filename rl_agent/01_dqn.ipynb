{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de3e60d",
   "metadata": {},
   "source": [
    "## DQNでエージェントを構築\n",
    "- Othelloの報酬設計と、環境は、utilsディレクトリ内に保存している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef50696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "from collections import deque\n",
    "import optuna\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "\n",
    "# ログ\n",
    "import logging\n",
    "\n",
    "# 学習やテスト用の関数\n",
    "from utils.othello_env import OthelloEnv\n",
    "from utils.othello_reward import ShapedReward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af76fb",
   "metadata": {},
   "source": [
    "## DQNのネットワーク定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba46330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=in_channels,\n",
    "            kernel_size=3, padding=1, groups=in_channels, bias=False,\n",
    "        )\n",
    "        self.pw = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=1, bias=False,\n",
    "        )\n",
    "        self.gn = nn.GroupNorm(num_groups=1, num_channels=out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x)\n",
    "        x = self.pw(x)\n",
    "        x = self.gn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=2, out_channels=8,\n",
    "                kernel_size=3, padding=1, bias=False,\n",
    "            ),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.block1 = DSConv(in_channels=8, out_channels=8)\n",
    "        self.block2 = DSConv(in_channels=8, out_channels=8)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=8, out_features=65),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(in_features=65, out_features=65)  # 65番目は、パスの行動\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc003cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DQN                                      [1, 65]                   --\n",
       "├─Sequential: 1-1                        [1, 8, 8, 8]              --\n",
       "│    └─Conv2d: 2-1                       [1, 8, 8, 8]              144\n",
       "│    └─SiLU: 2-2                         [1, 8, 8, 8]              --\n",
       "├─DSConv: 1-2                            [1, 8, 8, 8]              --\n",
       "│    └─Conv2d: 2-3                       [1, 8, 8, 8]              72\n",
       "│    └─Conv2d: 2-4                       [1, 8, 8, 8]              64\n",
       "│    └─GroupNorm: 2-5                    [1, 8, 8, 8]              16\n",
       "│    └─SiLU: 2-6                         [1, 8, 8, 8]              --\n",
       "├─DSConv: 1-3                            [1, 8, 8, 8]              --\n",
       "│    └─Conv2d: 2-7                       [1, 8, 8, 8]              72\n",
       "│    └─Conv2d: 2-8                       [1, 8, 8, 8]              64\n",
       "│    └─GroupNorm: 2-9                    [1, 8, 8, 8]              16\n",
       "│    └─SiLU: 2-10                        [1, 8, 8, 8]              --\n",
       "├─AdaptiveAvgPool2d: 1-4                 [1, 8, 1, 1]              --\n",
       "├─Sequential: 1-5                        [1, 65]                   --\n",
       "│    └─Flatten: 2-11                     [1, 8]                    --\n",
       "│    └─Linear: 2-12                      [1, 65]                   585\n",
       "│    └─SiLU: 2-13                        [1, 65]                   --\n",
       "├─Linear: 1-6                            [1, 65]                   4,290\n",
       "==========================================================================================\n",
       "Total params: 5,323\n",
       "Trainable params: 5,323\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# アーキテクチャのテスト\n",
    "dqn = DQN()\n",
    "dummy_board = torch.zeros((1, 1, 8, 8))\n",
    "dummy_player = torch.ones((1, 1, 8, 8))  # 手番 +1\n",
    "dummy_input = torch.cat([dummy_board, dummy_player], dim=1)  # (1,2,8,8)\n",
    "print(dqn(dummy_input).shape)\n",
    "torchinfo.summary(dqn, (1, 2, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00283a17",
   "metadata": {},
   "source": [
    "## DQNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fb3813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2df06871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDoubleDQN:\n",
    "    \"\"\"Double DQN の学習クラス\n",
    "\n",
    "    Double DQN による Q 学習を実装します。経験再生、ターゲットネット更新、\n",
    "    ε-greedy 探索を備え、直近 N エピソードの平均報酬が改善したときに\n",
    "    ベストモデルを保持します。\n",
    "\n",
    "    Args:\n",
    "        dqn (nn.Module): Qネットワーク（入力: (B,2,8,8), 出力: (B,65)）\n",
    "        gamma (float): 割引率。Defaults to ``0.99``.\n",
    "        lr (float): 学習率。Defaults to ``1e-3``.\n",
    "        batch_size (int): バッチサイズ。Defaults to ``64``.\n",
    "        init_memory_size (int): リプレイ初期化のために集める遷移数。Defaults to ``5000``.\n",
    "        memory_size (int): リプレイバッファ容量。Defaults to ``50000``.\n",
    "        target_update_freq (int): ハードターゲット更新の頻度（更新回数）。Defaults to ``1000``.\n",
    "        tau (float): ソフト更新係数（>0 で Polyak）。``0.0`` ならハード更新。Defaults to ``0.0``.\n",
    "        num_episodes (int): 学習エピソード数。Defaults to ``1000``.\n",
    "        max_steps_per_episode (int): 1エピソードの最大ステップ。Defaults to ``120``.\n",
    "        train_freq (int): 何環境ステップごとに学習を走らせるか。Defaults to ``1``.\n",
    "        gradient_steps (int): 1回の学習トリガでの更新回数。Defaults to ``1``.\n",
    "        learning_starts (int): これ未満は学習を行わない。Defaults to ``1000``.\n",
    "        epsilon_start (float): ε-greedy 初期値。Defaults to ``1.0``.\n",
    "        epsilon_end (float): ε-greedy 最終値。Defaults to ``0.05``.\n",
    "        epsilon_decay_steps (int): ε を線形減衰させるステップ数。Defaults to ``30000``.\n",
    "        seed (int): 乱数シード。Defaults to ``42``.\n",
    "        device (torch.device): 使用デバイス。None なら自動選択。\n",
    "        ReplayBufferCls (type): リプレイバッファクラス（append/sample 実装必須）。\n",
    "        shaping_eta (float): ポテンシャルシェーピング係数 η。Defaults to ``0.1``.\n",
    "        rolling_window (int): ベスト判定のローリング平均窓幅。Defaults to ``100``.\n",
    "        save_best_path (str): ベスト更新ごとに保存するパス。None なら保存なし。\n",
    "\n",
    "    Attributes:\n",
    "        rewards (List[float]): 各エピソードの合計報酬ログ。\n",
    "        losses (List[float]): 各更新の損失ログ。\n",
    "        best_state_dict (Dict[str, torch.Tensor]): ベストモデルの重み。\n",
    "        best_score (float): ローリング平均のベストスコア。\n",
    "    \"\"\"\n",
    "\n",
    "    # =============================\n",
    "    # 初期化\n",
    "    # =============================\n",
    "    def __init__(\n",
    "        self,\n",
    "        dqn,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        init_memory_size: int = 5000,\n",
    "        memory_size: int = 50000,\n",
    "        target_update_freq: int = 1000,\n",
    "        tau: float = 0.0,\n",
    "        num_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 120,\n",
    "        train_freq: int = 1,\n",
    "        gradient_steps: int = 1,\n",
    "        learning_starts: int = 1000,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.05,\n",
    "        epsilon_decay_steps: int = 30000,\n",
    "        seed: int = 42,\n",
    "        device: Optional[torch.device] = None,\n",
    "        ReplayBufferCls=None,\n",
    "        shaping_eta: float = 0.1,\n",
    "        rolling_window: int = 100,\n",
    "        save_best_path: Optional[str] = None,\n",
    "    ):\n",
    "        assert ReplayBufferCls is not None, \"ReplayBufferCls を渡してください\"\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # 乱数シード固定（再現性）\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # オンライン/ターゲットネットの構築\n",
    "        self.dqn = dqn.to(self.device)\n",
    "        self.target_dqn = copy.deepcopy(dqn).to(self.device)\n",
    "        self.target_dqn.eval()\n",
    "\n",
    "        # 最適化器・損失関数\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.SmoothL1Loss()  # Huber\n",
    "\n",
    "        # ハイパーパラメータ類\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.init_memory_size = init_memory_size\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.num_episodes = num_episodes\n",
    "        self.train_freq = max(1, int(train_freq))\n",
    "        self.gradient_steps = max(1, int(gradient_steps))\n",
    "        self.learning_starts = int(learning_starts)\n",
    "\n",
    "        # ターゲット更新関連\n",
    "        self.tau = float(tau)\n",
    "        self.target_update_freq = int(target_update_freq)\n",
    "        self._num_updates = 0  # optimizer.step() 回数\n",
    "        self._num_env_steps = 0  # 環境ステップ数（ε 減衰にも使用）\n",
    "\n",
    "        # ε-greedy パラメータ\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = max(1, int(epsilon_decay_steps))\n",
    "\n",
    "        # 経験再生\n",
    "        self.replay_buffer = ReplayBufferCls(memory_size)\n",
    "\n",
    "        # shaping 係数\n",
    "        self.shaping_eta = float(shaping_eta)\n",
    "\n",
    "        # ログ/ベスト追跡\n",
    "        self.rewards: List[float] = []\n",
    "        self.losses: List[float] = []\n",
    "        self.rolling_window = int(rolling_window)\n",
    "        self.best_score = -float(\"inf\")\n",
    "        self.best_state_dict = copy.deepcopy(self.dqn.state_dict())\n",
    "        self.save_best_path = save_best_path\n",
    "\n",
    "        # ランダムプレイでリプレイを初期化\n",
    "        self._init_replay_buffer()\n",
    "\n",
    "    # =============================\n",
    "    # 公開 API\n",
    "    # =============================\n",
    "    def train(self, return_best_model: bool = False):\n",
    "        \"\"\"指定エピソード数だけ学習を実行し、必要ならベストモデルも返す。\n",
    "\n",
    "        エピソードを順に実行し、経験を蓄積・学習します。ローリング平均で\n",
    "        ベストが更新されたら状態辞書を保持し、オプションでファイルにも保存します。\n",
    "\n",
    "        Args:\n",
    "            return_best_model (bool, optional): True の場合、metrics とともに\n",
    "                ベストモデル（deepcopy 済み）を返します。\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[float]] | Tuple[Dict[str, List[float]], nn.Module]:\n",
    "                メトリクス辞書（rewards, losses）。return_best_model=True なら\n",
    "                これに加えてベストモデルを返します。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=self.num_episodes, desc=\"Train Double DQN\")\n",
    "        for ep in range(self.num_episodes):\n",
    "            # 1 エピソード実行\n",
    "            ep_reward = self._run_episode(ep)\n",
    "            self.rewards.append(ep_reward)\n",
    "\n",
    "            # 直近ローリング平均（窓幅に達するまでは全体平均）\n",
    "            if len(self.rewards) >= self.rolling_window:\n",
    "                rolling_avg = float(np.mean(self.rewards[-self.rolling_window:]))\n",
    "            else:\n",
    "                rolling_avg = float(np.mean(self.rewards))\n",
    "\n",
    "            # ベスト更新判定\n",
    "            if rolling_avg > self.best_score:\n",
    "                self.best_score = rolling_avg\n",
    "                self.best_state_dict = copy.deepcopy(self.dqn.state_dict())\n",
    "                if self.save_best_path is not None:\n",
    "                    torch.save(self.best_state_dict, self.save_best_path)\n",
    "\n",
    "            # 進捗バーの表示\n",
    "            last_loss = self.losses[-1] if self.losses else float(\"nan\")\n",
    "            pbar.set_postfix_str(\n",
    "                f\"Episode Reward: {ep_reward:.2f}  Roll@{self.rolling_window}: {rolling_avg:.2f}  \"\n",
    "                f\"Best: {self.best_score:.2f}  Loss: {last_loss:.3f}  ε: {self._epsilon_by_step(self._num_env_steps):.3f}\"\n",
    "            )\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        metrics = {\"rewards\": self.rewards, \"losses\": self.losses}\n",
    "        if return_best_model:\n",
    "            best_model = self._clone_model_with_state(self.best_state_dict)\n",
    "            return metrics, best_model\n",
    "        return metrics\n",
    "\n",
    "    def get_best_model(self) -> nn.Module:\n",
    "        \"\"\"保持しているベストの重みで初期化したモデルを返す。\n",
    "\n",
    "        Returns:\n",
    "            nn.Module: deepcopy 済みのモデルに best_state_dict を load して eval() 済み。\n",
    "        \"\"\"\n",
    "        return self._clone_model_with_state(self.best_state_dict)\n",
    "\n",
    "    # =============================\n",
    "    # 内部ヘルパ\n",
    "    # =============================\n",
    "    def _clone_model_with_state(self, state_dict) -> nn.Module:\n",
    "        \"\"\"オンラインネットのクローンを作成し、指定の重みを読み込む。\n",
    "\n",
    "        Args:\n",
    "            state_dict (Dict[str, torch.Tensor]): ロードする重み。\n",
    "\n",
    "        Returns:\n",
    "            nn.Module: device 上に配置され eval() 済みのコピー。\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(self.dqn).to(self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def _run_episode(self, episode_idx: int) -> float:\n",
    "        \"\"\"1 エピソード分の相互作用を実行して合計報酬を返す。\n",
    "\n",
    "        環境に対して ε-greedy で行動し、経験をリプレイバッファへ保存。\n",
    "        train_freq ごとに学習を走らせ、必要に応じてターゲット更新を行います。\n",
    "\n",
    "        Args:\n",
    "            episode_idx (int): エピソード番号（ログ/拡張用）。\n",
    "\n",
    "        Returns:\n",
    "            float: 当該エピソードの合計報酬。\n",
    "        \"\"\"\n",
    "        env = OthelloEnv()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done and steps < self.max_steps_per_episode:\n",
    "            # ε-greedy で行動選択\n",
    "            epsilon = self._epsilon_by_step(self._num_env_steps)\n",
    "            player = env.player  # 現在手番（+1/-1）\n",
    "            state = env.get_state()  # (8,8) float32\n",
    "            action = self._select_action_by_epsilon_greedy(env, epsilon)\n",
    "\n",
    "            # シェーピング報酬：prev/next から r を算出\n",
    "            shaped = ShapedReward(player, eta=self.shaping_eta, gamma=self.gamma)\n",
    "            next_state, reward, done, _ = env.step(action, reward_fn=shaped.get_reward)\n",
    "            next_player = env.player\n",
    "\n",
    "            # 次状態の合法手とともに遷移を保存\n",
    "            next_legal_actions = self._legal_actions_from_board(next_state, next_player)\n",
    "            self._store_transition(\n",
    "                board=state,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                next_board=next_state,\n",
    "                done=done,\n",
    "                player=player,\n",
    "                next_player=next_player,\n",
    "                next_legal_actions=next_legal_actions,\n",
    "            )\n",
    "\n",
    "            # バッファが暖まっていれば学習を実行\n",
    "            if (self._num_env_steps % self.train_freq == 0) and (len(self.replay_buffer) >= max(self.batch_size, self.learning_starts)):\n",
    "                for _ in range(self.gradient_steps):\n",
    "                    loss = self._update_dqn_double()\n",
    "                    if not math.isnan(loss):\n",
    "                        self.losses.append(loss)\n",
    "\n",
    "            # 必要ならターゲット更新\n",
    "            self._maybe_update_target()\n",
    "\n",
    "            total_reward += float(reward)\n",
    "            steps += 1\n",
    "            self._num_env_steps += 1\n",
    "\n",
    "        return float(total_reward)\n",
    "\n",
    "    def _update_dqn_double(self) -> float:\n",
    "        \"\"\"Double DQN の 1 回分の更新を実行して損失を返す。\n",
    "\n",
    "        next 状態でオンラインネットにより argmax を選び、ターゲットネットで\n",
    "        その値を評価（Double DQN）。かつ、合法手のみを有効化するマスクを適用します。\n",
    "\n",
    "        Returns:\n",
    "            float: Huber 損失のスカラー値。\n",
    "        \"\"\"\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        boards_np = np.stack([b['board'] for b in batch])              # (B,8,8)\n",
    "        next_boards_np = np.stack([b['next_board'] for b in batch])    # (B,8,8)\n",
    "        players_np = np.array([b['player'] for b in batch], dtype=np.float32)\n",
    "        next_players_np = np.array([b['next_player'] for b in batch], dtype=np.float32)\n",
    "\n",
    "        board = self._batch_to_input(boards_np, players_np)            # (B,2,8,8)\n",
    "        next_board = self._batch_to_input(next_boards_np, next_players_np)\n",
    "\n",
    "        # (B,2,8,8) テンソルを構築（チャネル0=盤面, チャネル1=手番）\n",
    "        board = torch.stack([self._to_input(b['board'], b['player']) for b in batch]).to(self.device)\n",
    "        next_board = torch.stack([self._to_input(b['next_board'], b['next_player']) for b in batch]).to(self.device)\n",
    "\n",
    "        # スカラー群\n",
    "        action = torch.tensor([b['action'] for b in batch], dtype=torch.int64, device=self.device)\n",
    "        reward = torch.tensor([b['reward'] for b in batch], dtype=torch.float32, device=self.device)\n",
    "        done = torch.tensor([b['done'] for b in batch], dtype=torch.float32, device=self.device)\n",
    "        next_legal_actions_list = [b['next_legal_actions'] for b in batch]\n",
    "\n",
    "        # Q(s,a) を抽出\n",
    "        q_all = self.dqn(board)                                 # (B,65)\n",
    "        q_sa = q_all.gather(1, action.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 次状態で非合法手をマスク（-1e9 加算）\n",
    "            next_masks = self._build_masks_from_indices(next_legal_actions_list, fill_value=-1e9)\n",
    "            q_next_online = self.dqn(next_board) + next_masks\n",
    "            next_actions_online = q_next_online.argmax(dim=1)  # (B,)\n",
    "\n",
    "            # ターゲットネットで評価\n",
    "            q_next_target = self.target_dqn(next_board)\n",
    "            next_q = q_next_target.gather(1, next_actions_online.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "\n",
    "            # ベルマンターゲット\n",
    "            target = reward + self.gamma * next_q * (1.0 - done)\n",
    "\n",
    "        loss = self.loss_fn(q_sa, target)\n",
    "        if torch.isnan(loss):  # まれな数値異常を検知\n",
    "            return float(\"nan\")\n",
    "\n",
    "        # 逆伝播と最適化\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dqn.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self._num_updates += 1\n",
    "        return float(loss.item())\n",
    "\n",
    "    def _select_action_by_epsilon_greedy(self, env, epsilon: float) -> int:\n",
    "        \"\"\"ε-greedy に基づき合法手の中から行動をサンプルする。\n",
    "\n",
    "        Args:\n",
    "            env (OthelloEnv): 環境インスタンス。\n",
    "            epsilon (float): 探索率 ε（0〜1）。\n",
    "\n",
    "        Returns:\n",
    "            int: 選択した行動インデックス（0..64）。\n",
    "        \"\"\"\n",
    "        legal_actions = env.legal_actions()\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(legal_actions)\n",
    "        return self._select_action_by_greedy(env)\n",
    "\n",
    "    def _select_action_by_greedy(self, env) -> int:\n",
    "        \"\"\"合法手のみを対象に Q を比較し、貪欲（argmax）に行動を選ぶ。\n",
    "\n",
    "        Args:\n",
    "            env (OthelloEnv): 環境インスタンス。\n",
    "\n",
    "        Returns:\n",
    "            int: 貪欲に選んだ行動インデックス（0..64）。\n",
    "        \"\"\"\n",
    "        legal_actions = env.legal_actions()\n",
    "        board_tensor = self._to_input(env.get_state(), env.player).unsqueeze(0).to(self.device)  # (1,2,8,8)\n",
    "        with torch.no_grad():\n",
    "            q_all = self.dqn(board_tensor).squeeze(0)  # (65,)\n",
    "            # 非合法手を巨大負値で潰す\n",
    "            mask = torch.full((65,), -1e9, device=self.device)\n",
    "            for a in legal_actions:\n",
    "                mask[a] = 0.0\n",
    "            q_masked = q_all + mask\n",
    "            action = int(q_masked.argmax().item())\n",
    "        return action\n",
    "\n",
    "    def _init_replay_buffer(self):\n",
    "        \"\"\"ランダムポリシーで遷移を収集し、リプレイバッファを初期化する。\"\"\"\n",
    "        target = min(self.init_memory_size, self.replay_buffer.memory_size)\n",
    "        added = 0\n",
    "        pbar = tqdm(total=target, desc='Init replay buffer')\n",
    "        while added < target:\n",
    "            env = OthelloEnv()\n",
    "            done = False\n",
    "            while not done and added < target:\n",
    "                player = env.player\n",
    "                state = env.get_state()\n",
    "                legal_actions = env.legal_actions()\n",
    "                action = random.choice(legal_actions)\n",
    "                shaped = ShapedReward(player, eta=self.shaping_eta, gamma=self.gamma)\n",
    "                next_state, reward, done, _ = env.step(action, reward_fn=shaped.get_reward)\n",
    "                next_player = env.player\n",
    "                next_legal_actions = self._legal_actions_from_board(next_state, next_player)\n",
    "\n",
    "                self._store_transition(\n",
    "                    board=state,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    next_board=next_state,\n",
    "                    done=done,\n",
    "                    player=player,\n",
    "                    next_player=next_player,\n",
    "                    next_legal_actions=next_legal_actions,\n",
    "                )\n",
    "                added += 1\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    def _maybe_update_target(self):\n",
    "        \"\"\"ターゲットネットを Polyak またはハードコピーで更新する。\n",
    "\n",
    "        * tau > 0 の場合: ``target = (1-tau)*target + tau*online``（ソフト更新）\n",
    "        * それ以外: optimizer の更新回数で target_update_freq ごとにハード更新\n",
    "        \"\"\"\n",
    "        if self.tau and self.tau > 0.0:\n",
    "            with torch.no_grad():\n",
    "                for tp, p in zip(self.target_dqn.parameters(), self.dqn.parameters()):\n",
    "                    tp.data.mul_(1.0 - self.tau).add_(self.tau * p.data)\n",
    "        else:\n",
    "            if (self._num_updates % max(1, self.target_update_freq)) == 0 and self._num_updates > 0:\n",
    "                self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _epsilon_by_step(self, step: int) -> float:\n",
    "        \"\"\"環境ステップ数に応じて ε を線形減衰させる。\n",
    "\n",
    "        Args:\n",
    "            step (int): 現在の環境ステップ数。\n",
    "\n",
    "        Returns:\n",
    "            float: [epsilon_end, epsilon_start] の範囲での ε 値。\n",
    "        \"\"\"\n",
    "        if step >= self.epsilon_decay_steps:\n",
    "            return self.epsilon_end\n",
    "        span = self.epsilon_start - self.epsilon_end\n",
    "        return self.epsilon_start - span * (step / self.epsilon_decay_steps)\n",
    "\n",
    "    def _build_masks_from_indices(self, batch_next_legal_actions, fill_value: float = -1e9):\n",
    "        \"\"\"次状態ごとの合法手リストから加算マスクを作る。\n",
    "\n",
    "        Args:\n",
    "            batch_next_legal_actions (List[List[int]]): 各バッチにおける合法手のリスト。\n",
    "            fill_value (float): 非合法手に与える値（例: -1e9）。Defaults to ``-1e9``.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 形状 (B,65) のマスクテンソル（合法手は 0、非合法手は fill_value）。\n",
    "        \"\"\"\n",
    "        B = len(batch_next_legal_actions)\n",
    "        masks = torch.full((B, 65), float(fill_value), device=self.device)\n",
    "        for i, acts in enumerate(batch_next_legal_actions):\n",
    "            for a in acts:\n",
    "                masks[i, a] = 0.0\n",
    "        return masks\n",
    "\n",
    "    def _to_input(self, board_like, player_scalar: int) -> torch.Tensor:\n",
    "        \"\"\"盤面と手番スカラーから 2 チャンネルテンソルを生成する。\n",
    "\n",
    "        チャネル0: 盤面（-1,0,+1）。チャネル1: 手番プレーン（+1/-1 で全面を塗る）。\n",
    "\n",
    "        Args:\n",
    "            board_like (array-like): 形状 (8,8) または (1,8,8) の配列。\n",
    "            player_scalar (int): +1（黒手番）または -1（白手番）。\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 形状 (2,8,8) の float32 テンソル。\n",
    "        \"\"\"\n",
    "        t = torch.as_tensor(board_like, dtype=torch.float32)\n",
    "        if t.dim() == 2 and t.shape == (8, 8):\n",
    "            t = t.unsqueeze(0)  # (1,8,8)\n",
    "        elif t.dim() == 3 and t.shape == (1, 8, 8):\n",
    "            pass\n",
    "        else:\n",
    "            t = t.reshape(1, 8, 8)\n",
    "        player_plane = torch.full_like(t, float(player_scalar))  # (1,8,8)\n",
    "        return torch.cat([t, player_plane], dim=0)  # (2,8,8)\n",
    "\n",
    "    def _batch_to_input(self, boards_np: np.ndarray, players_np: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        boards_np: (B, 8, 8), players_np: (B,)\n",
    "        \"\"\"\n",
    "        t = torch.as_tensor(boards_np, dtype=torch.float32, device=self.device).unsqueeze(1)  # (B,1,8,8)\n",
    "        p = torch.as_tensor(players_np, dtype=torch.float32, device=self.device).view(-1,1,1,1).expand(-1,1,8,8)\n",
    "        return torch.cat([t, p], dim=1)  # (B,2,8,8)\n",
    "\n",
    "    def _legal_actions_from_board(self, board_np: np.ndarray, player: int) -> List[int]:\n",
    "        \"\"\"次状態の盤面と手番から合法手（インデックス配列）を再計算する。\n",
    "\n",
    "        Args:\n",
    "            board_np (np.ndarray): 形状 (8,8) の盤面（np.float32 互換）。\n",
    "            player (int): +1（黒）または -1（白）。\n",
    "\n",
    "        Returns:\n",
    "            List[int]: 合法手のインデックス。合法手が無ければ [64]（パス）。\n",
    "        \"\"\"\n",
    "        # ここでは副作用を避けるため、OthelloGame を新規に作り直す\n",
    "        g = OthelloEnv().game.__class__()  # 既存環境からゲームクラスを取得して生成\n",
    "        g.board = board_np.astype(np.int8).tolist()\n",
    "        g.player = player\n",
    "        moves = g.legal_moves(player)\n",
    "        if not moves:\n",
    "            return [64]\n",
    "        return [r * 8 + c for r, c in moves]\n",
    "\n",
    "    def _store_transition(\n",
    "        self,\n",
    "        board,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_board,\n",
    "        done: bool,\n",
    "        player: int,\n",
    "        next_player: int,\n",
    "        next_legal_actions: List[int],\n",
    "    ):\n",
    "        \"\"\"1 ステップ分の遷移をリプレイバッファに保存する。\n",
    "\n",
    "        Args:\n",
    "            board (np.ndarray): 現在の盤面 (8,8)。\n",
    "            action (int): 実行した行動インデックス。\n",
    "            reward (float): 即時報酬。\n",
    "            next_board (np.ndarray): 次の盤面 (8,8)。\n",
    "            done (bool): 終局フラグ。\n",
    "            player (int): 現在盤面での手番（+1/-1）。\n",
    "            next_player (int): 次盤面での手番（+1/-1）。\n",
    "            next_legal_actions (List[int]): 次状態での合法手インデックス一覧。\n",
    "        \"\"\"\n",
    "        transition = {\n",
    "            \"board\": np.array(board, dtype=np.float32),\n",
    "            \"action\": int(action),\n",
    "            \"reward\": float(reward),\n",
    "            \"next_board\": np.array(next_board, dtype=np.float32),\n",
    "            \"done\": bool(done),\n",
    "            \"player\": int(player),\n",
    "            \"next_player\": int(next_player),\n",
    "            \"next_legal_actions\": next_legal_actions,\n",
    "        }\n",
    "        self.replay_buffer.append(transition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c52fdaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b82e5e1d2de48e2aa03dcd539485450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Init replay buffer:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bb31c2bda74f33a42a2b6441ad92bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Double DQN:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     16\u001b[39m trainer = TrainDoubleDQN(\n\u001b[32m     17\u001b[39m     dqn=dqn,\n\u001b[32m     18\u001b[39m     device=device,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     save_best_path=\u001b[33m\"\u001b[39m\u001b[33mbest_dqn.pth\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# ベスト更新のたび保存（不要なら None）\u001b[39;00m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 学習開始：ベストモデルも受け取る\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m metrics, best_model = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 学習曲線を確認\u001b[39;00m\n\u001b[32m     38\u001b[39m plt.figure()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mTrainDoubleDQN.train\u001b[39m\u001b[34m(self, return_best_model)\u001b[39m\n\u001b[32m    140\u001b[39m pbar = tqdm(total=\u001b[38;5;28mself\u001b[39m.num_episodes, desc=\u001b[33m\"\u001b[39m\u001b[33mTrain Double DQN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_episodes):\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# 1 エピソード実行\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     ep_reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m.rewards.append(ep_reward)\n\u001b[32m    146\u001b[39m     \u001b[38;5;66;03m# 直近ローリング平均（窓幅に達するまでは全体平均）\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 244\u001b[39m, in \u001b[36mTrainDoubleDQN._run_episode\u001b[39m\u001b[34m(self, episode_idx)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._num_env_steps % \u001b[38;5;28mself\u001b[39m.train_freq == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.replay_buffer) >= \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.batch_size, \u001b[38;5;28mself\u001b[39m.learning_starts)):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.gradient_steps):\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_dqn_double\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math.isnan(loss):\n\u001b[32m    246\u001b[39m             \u001b[38;5;28mself\u001b[39m.losses.append(loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 311\u001b[39m, in \u001b[36mTrainDoubleDQN._update_dqn_double\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    309\u001b[39m loss.backward()\n\u001b[32m    310\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.dqn.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[38;5;28mself\u001b[39m._num_updates += \u001b[32m1\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/othello-with-rl/rl_agent/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/othello-with-rl/rl_agent/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/othello-with-rl/rl_agent/.venv/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/othello-with-rl/rl_agent/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/othello-with-rl/rl_agent/.venv/lib/python3.12/site-packages/torch/optim/adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/othello-with-rl/rl_agent/.venv/lib/python3.12/site-packages/torch/optim/adam.py:464\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    462\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    467\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# デバイス自動選択\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# DQN を作成（あなたの2ch対応版）\n",
    "dqn = DQN().to(device)\n",
    "\n",
    "# リプレイバッファ\n",
    "replay_cls = ReplayBuffer\n",
    "\n",
    "# ハイパラ（まずは軽めに）\n",
    "trainer = TrainDoubleDQN(\n",
    "    dqn=dqn,\n",
    "    device=device,\n",
    "    ReplayBufferCls=replay_cls,\n",
    "    num_episodes=10,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    target_update_freq=200,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay_steps=1000,\n",
    "    max_steps_per_episode=120,\n",
    "    shaping_eta=0.1,\n",
    "    rolling_window=100,\n",
    "    save_best_path=\"best_dqn.pth\",  # ベスト更新のたび保存（不要なら None）\n",
    ")\n",
    "\n",
    "# 学習開始：ベストモデルも受け取る\n",
    "metrics, best_model = trainer.train(return_best_model=True)\n",
    "\n",
    "# 学習曲線を確認\n",
    "plt.figure()\n",
    "plt.plot(metrics[\"rewards\"])\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "# ベストモデルの重みを保存\n",
    "# ファイル名は、タイムスタンプを含める\n",
    "save_dir = \"model_weights/ddqn\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_weights_path = f\"{save_dir}/{timestamp}.pth\"\n",
    "torch.save(best_model.state_dict(), save_weights_path)\n",
    "print(f\"Best model saved to {save_weights_path}\")\n",
    "\n",
    "# ベスト重みでオンラインネットを上書き（以降の推論に反映したい場合）\n",
    "trainer.dqn.load_state_dict(best_model.state_dict())\n",
    "trainer.dqn.eval()\n",
    "print(\"Best model loaded into trainer.dqn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
