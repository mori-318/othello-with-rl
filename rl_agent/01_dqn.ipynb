{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de3e60d",
   "metadata": {},
   "source": [
    "## DQNでエージェントを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef50696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "from collections import deque\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ee691",
   "metadata": {},
   "source": [
    "## オセロ環境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292c73d",
   "metadata": {},
   "source": [
    "### オセロゲームクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca48b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY, BLACK, WHITE = 0, 1, -1\n",
    "DIRECTIONS = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "\n",
    "class OthelloGame:\n",
    "    def __init__(self):\n",
    "        self.board = [[EMPTY for _ in range(8)] for _ in range(8)]\n",
    "        self.board[3][3] = self.board[4][4] = WHITE\n",
    "        self.board[3][4] = self.board[4][3] = BLACK\n",
    "        self.player = BLACK\n",
    "\n",
    "    @staticmethod\n",
    "    def opponent(player: int) -> int:\n",
    "        \"\"\"\n",
    "        現在のプレイヤーの相手を返す\n",
    "        Args:\n",
    "            player(int): 現在のプレイヤー\n",
    "        Returns:\n",
    "            int: 相手のプレイヤー\n",
    "        Examples:\n",
    "            >>> OthelloGame.opponent(1)\n",
    "            -1\n",
    "            >>> OthelloGame.opponent(-1)\n",
    "            1\n",
    "        \"\"\"\n",
    "        return -player\n",
    "\n",
    "    def clone(self):\n",
    "        \"\"\"盤面をコピーして新しいインスタンスを返す\"\"\"\n",
    "        g = OthelloGame()\n",
    "        g.board = [row[:] for row in self.board]\n",
    "        g.player = self.player\n",
    "        return g\n",
    "\n",
    "    def inside(self, r, c):\n",
    "        \"\"\"\n",
    "        盤面の範囲内かを返す\n",
    "        Args:\n",
    "            r (int): 行\n",
    "            c (int): 列\n",
    "        Returns:\n",
    "            bool: 盤面の範囲内か\n",
    "        \"\"\"\n",
    "        return 0 <= r < 8 and 0 <= c < 8\n",
    "\n",
    "    def legal_moves(self, player=None) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        合法手を返す\n",
    "        Args:\n",
    "            player (int, optional): プレイヤーの色.指定しない場合は現在のプレイヤーを返す\n",
    "        Returns:\n",
    "            List[Tuple[int, int]]: 合法手のリスト\n",
    "        \"\"\"\n",
    "        if player is None:\n",
    "            player = self.player\n",
    "        moves = []\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if self.board[r][c] != EMPTY:  # 空白でない場合は合法手ではない\n",
    "                    continue\n",
    "                if self._would_flip(r, c, player):  # 石をひっくり返せる場合は合法手\n",
    "                    moves.append((r, c))\n",
    "        return moves\n",
    "\n",
    "    def _would_flip(self, r, c, player) -> bool:\n",
    "        \"\"\"\n",
    "        石をひっくり返せるかを返す\n",
    "        Args:\n",
    "            r (int): 行\n",
    "            c (int): 列\n",
    "            player (int): プレイヤーの色\n",
    "        Returns:\n",
    "            bool: 石をひっくり返せるか\n",
    "        \"\"\"\n",
    "        if self.board[r][c] != EMPTY:  # 空白でない場合は石をひっくり返せない\n",
    "            return False\n",
    "        for dr, dc in DIRECTIONS:\n",
    "            rr, cc = r + dr, c + dc\n",
    "            seen_opp = False  # 相手の石を一度見たか\n",
    "            while self.inside(rr, cc) and self.board[rr][cc] == self.opponent(player):  # 盤面の範囲内か & 相手の石が置かれている場所か\n",
    "                seen_opp = True\n",
    "                rr += dr; cc += dc\n",
    "            if seen_opp and self.inside(rr, cc) and self.board[rr][cc] == player:  # 相手の石を一度見たか & 盤面の範囲内か & 自分の石をみたか\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def play(self, r, c, player=None):\n",
    "        \"\"\"\n",
    "        石を置く & 石をひっくり返す\n",
    "        Args:\n",
    "            r (int): 行\n",
    "            c (int): 列\n",
    "            player (int, optional): プレイヤーの色.指定しない場合は現在のプレイヤーを返す\n",
    "        \"\"\"\n",
    "        if player is None:\n",
    "            player = self.player\n",
    "        assert self.board[r][c] == EMPTY  # 空白でない場合は石を置くことができない\n",
    "        flipped = []\n",
    "        for dr, dc in DIRECTIONS:\n",
    "            line = []\n",
    "            rr, cc = r + dr, c + dc\n",
    "            while self.inside(rr,cc) and self.board[rr][cc] == self.opponent(player):  # 盤面の範囲内か & 相手の石が置かれている場所か\n",
    "                line.append((rr,cc))  # 石をひっくり返す場所を追加\n",
    "                rr += dr; cc += dc\n",
    "            if line and self.inside(rr,cc) and self.board[rr][cc] == player:  # 石をひっくり返す場所が存在 & 盤面の範囲内か & 自分の石をみたか\n",
    "                flipped.extend(line)  # 石をひっくり返す場所を追加\n",
    "        if not flipped:  # 石をひっくり返す場所が存在しない場合は不正な手\n",
    "            raise ValueError(\"Illegal move\")\n",
    "        self.board[r][c] = player\n",
    "        for rr,cc in flipped:  # 石をひっくり返す\n",
    "            self.board[rr][cc] = player\n",
    "        self.player = self.opponent(player)\n",
    "        # 現在のプレイヤーが合法手がない場合は相手の番\n",
    "        if not self.legal_moves(self.player):\n",
    "            self.player = self.opponent(self.player)\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        \"\"\"\n",
    "        終局かを返す\n",
    "        Returns:\n",
    "            bool: 終局か\n",
    "        \"\"\"\n",
    "        if self.legal_moves(BLACK): return False\n",
    "        if self.legal_moves(WHITE): return False\n",
    "        return True\n",
    "\n",
    "    def game_score(self) -> int:\n",
    "        \"\"\"\n",
    "        黒が+1, 白が-1としてスコアを計算\n",
    "        Returns:\n",
    "            int: スコア\n",
    "        \"\"\"\n",
    "        s = 0\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                s += self.board[r][c]\n",
    "        return s\n",
    "\n",
    "    def winner(self) -> int:\n",
    "        s = self.game_score()\n",
    "        return BLACK if s > 0 else WHITE if s < 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0093b77d",
   "metadata": {},
   "source": [
    "### 報酬設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b156d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloPotential:\n",
    "    def __init__(\n",
    "        self,\n",
    "        player: int,\n",
    "        w_corner: float = 1.0,\n",
    "        w_mob: float = 0.7,\n",
    "        w_frontier: float = -0.4\n",
    "    ):\n",
    "        self.player = player\n",
    "        self.w_corner = w_corner\n",
    "        self.w_mob = w_mob\n",
    "        self.w_frontier = w_frontier\n",
    "\n",
    "    def phi(self, env: OthelloGame) -> float:\n",
    "        corner_term = self._ratio(*self._count_corner(env))\n",
    "        mob_term = self._ratio(\n",
    "            len(env.legal_moves(self.player)),\n",
    "            len(env.legal_moves(env.opponent(self.player)))\n",
    "        )\n",
    "        front_term = self._ratio(*self._count_frontier(env))\n",
    "        return self.w_corner*corner_term + self.w_mob*mob_term + self.w_frontier*front_term\n",
    "\n",
    "    def terminal_reward(self, env: OthelloGame) -> float:\n",
    "        if not env.is_terminal():\n",
    "            return 0.0\n",
    "        pc, oc = self._count_stones(env)\n",
    "        return 1.0 if pc > oc else -1.0 if pc < oc else 0.0\n",
    "\n",
    "    # --- helpers ---\n",
    "    def _ratio(self, a: int, b: int) -> float:\n",
    "        s = a + b\n",
    "        return (a - b) / max(1, s)\n",
    "\n",
    "    def _count_stones(self, env: OthelloGame) -> Tuple[int, int]:\n",
    "        pc = oc = 0\n",
    "        opp = env.opponent(self.player)\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if env.board[r][c] == self.player: pc += 1\n",
    "                elif env.board[r][c] == opp: oc += 1\n",
    "        return pc, oc\n",
    "\n",
    "    def _count_corner(self, env: OthelloGame) -> Tuple[int, int]:\n",
    "        corners = [(0,0),(0,7),(7,0),(7,7)]\n",
    "        pc = oc = 0\n",
    "        opp = env.opponent(self.player)\n",
    "        for r,c in corners:\n",
    "            if env.board[r][c] == self.player: pc += 1\n",
    "            elif env.board[r][c] == opp: oc += 1\n",
    "        return pc, oc\n",
    "\n",
    "    def _count_frontier(self, env: OthelloGame) -> Tuple[int, int]:\n",
    "        pc = oc = 0\n",
    "        opp = env.opponent(self.player)\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                cell = env.board[r][c]\n",
    "                if cell == EMPTY:\n",
    "                    continue\n",
    "                frontier = any(\n",
    "                    0 <= r+dr < 8 and 0 <= c+dc < 8 and env.board[r+dr][c+dc] == EMPTY\n",
    "                    for dr in (-1,0,1) for dc in (-1,0,1) if dr or dc\n",
    "                )\n",
    "                if frontier:\n",
    "                    if cell == self.player: pc += 1\n",
    "                    elif cell == opp: oc += 1\n",
    "        return pc, oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ab1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapedReward:\n",
    "    \"\"\"\n",
    "    状態価値を用いた報酬を算出\n",
    "    r = r_terminal(s') + eta * (gamma * Φ(s') - Φ(s))\n",
    "    \"\"\"\n",
    "    def __init__(self, player: int, eta: float = 0.1, gamma: float = 0.99):\n",
    "        self.player = player\n",
    "        self.eta = float(eta)\n",
    "        self.gamma = float(gamma)\n",
    "        self.potential = OthelloPotential(player)\n",
    "\n",
    "    def get_reward(self, prev_env: OthelloGame, next_env: OthelloGame) -> float:\n",
    "        r_term = self.potential.terminal_reward(next_env)\n",
    "        phi_prev = self.potential.phi(prev_env)\n",
    "        phi_next = self.potential.phi(next_env)\n",
    "        shaped = self.gamma * phi_next - phi_prev\n",
    "        return r_term + self.eta * shaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ed4ef",
   "metadata": {},
   "source": [
    "### Gym風API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86275184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloEnv:\n",
    "    \"\"\"\n",
    "    Othello (Reversi) 環境の Gym 風クラス。\n",
    "\n",
    "    このクラスは強化学習のための環境インターフェースを提供します。\n",
    "    盤面の初期化、合法手の生成、行動適用、報酬計算、終局判定を行います。\n",
    "\n",
    "    Attributes:\n",
    "        game (Othello): ゲーム状態を保持する Othello インスタンス。\n",
    "        player (int): 現在のプレイヤー (BLACK=1, WHITE=-1)。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" コンストラクタ \"\"\"\n",
    "        self.game = OthelloGame()\n",
    "        self.player = self.game.player\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        環境を初期状態にリセットし、観測を返す\n",
    "        Returns:\n",
    "            observation (np.ndarray): 盤面の状態 (8x8の配列)\n",
    "        \"\"\"\n",
    "        self.game = OthelloGame()\n",
    "        self.player = self.game.player\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action, reward_fn=None):\n",
    "        \"\"\"\n",
    "        １ステップ環境を進める\n",
    "        Args:\n",
    "            action (int): 0~64の整数（64はパス）\n",
    "            reward_fn (Reward, Optional): 報酬関数のインスタンス\n",
    "                指定された場合、行動適用後に報酬を計算\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - observation (np.ndarray): 次の盤面の状態 (8x8, float32)\n",
    "                - reward (float): 報酬\n",
    "                - done (bool): ゲーム終了フラグ\n",
    "                - info (dict): 追加情報\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "\n",
    "        prev_game = self.game.clone()\n",
    "\n",
    "        try:\n",
    "            if action == 64:  # パスの場合\n",
    "                # パスして相手番\n",
    "                self.game.player = self.game.opponent(self.game.player)\n",
    "            else:\n",
    "                r, c = divmod(action, 8)\n",
    "                self.game.play(r, c, self.game.player)\n",
    "        except ValueError:\n",
    "            # 不正手なら、即終了 & 負の報酬\n",
    "            done = True\n",
    "            reward = -10.0\n",
    "            return self.get_state(), reward, done, {}\n",
    "\n",
    "        # 報酬を計算\n",
    "        if reward_fn is not None:\n",
    "            reward = reward_fn(prev_game, self.game)\n",
    "\n",
    "        done = self.game.is_terminal()\n",
    "        self.player = self.game.player\n",
    "\n",
    "        return self.get_state(), reward, done, {}\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        現在の盤面をnumpy配列で取得\n",
    "        Returns:\n",
    "            observation (np.ndarray): 盤面の状態 (8x8, float32)\n",
    "        \"\"\"\n",
    "        return np.array(self.game.board, dtype=np.float32)\n",
    "\n",
    "    def legal_actions(self):\n",
    "        \"\"\"\n",
    "        現在のプレイヤーの合法手を整数インデックスのリストとして返す\n",
    "        Returns:\n",
    "            actions (list): 合法手の整数インデックスのリスト\n",
    "        \"\"\"\n",
    "        moves = self.game.legal_moves(self.player)\n",
    "        if not moves:\n",
    "            return [64]\n",
    "        return [r * 8 + c for r, c in moves]\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        盤面をコンソールに出力する\n",
    "        \"\"\"\n",
    "        board_np = np.array(self.game.board)\n",
    "        print(board_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af76fb",
   "metadata": {},
   "source": [
    "## DQNのネットワーク定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba46330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=in_channels,\n",
    "            kernel_size=3, padding=1, groups=in_channels, bias=False,\n",
    "        )\n",
    "        self.pw = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=1, bias=False,\n",
    "        )\n",
    "        self.gn = nn.GroupNorm(num_groups=1, num_channels=out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x)\n",
    "        x = self.pw(x)\n",
    "        x = self.gn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=2, out_channels=8,\n",
    "                kernel_size=3, padding=1, bias=False,\n",
    "            ),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.block1 = DSConv(in_channels=8, out_channels=8)\n",
    "        self.block2 = DSConv(in_channels=8, out_channels=8)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=8, out_features=65),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(in_features=65, out_features=65)  # 65番目は、パスの行動\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc003cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DQN                                      [1, 65]                   --\n",
       "├─Sequential: 1-1                        [1, 8, 8, 8]              --\n",
       "│    └─Conv2d: 2-1                       [1, 8, 8, 8]              144\n",
       "│    └─SiLU: 2-2                         [1, 8, 8, 8]              --\n",
       "├─DSConv: 1-2                            [1, 8, 8, 8]              --\n",
       "│    └─Conv2d: 2-3                       [1, 8, 8, 8]              72\n",
       "│    └─Conv2d: 2-4                       [1, 8, 8, 8]              64\n",
       "│    └─GroupNorm: 2-5                    [1, 8, 8, 8]              16\n",
       "│    └─SiLU: 2-6                         [1, 8, 8, 8]              --\n",
       "├─DSConv: 1-3                            [1, 8, 8, 8]              --\n",
       "│    └─Conv2d: 2-7                       [1, 8, 8, 8]              72\n",
       "│    └─Conv2d: 2-8                       [1, 8, 8, 8]              64\n",
       "│    └─GroupNorm: 2-9                    [1, 8, 8, 8]              16\n",
       "│    └─SiLU: 2-10                        [1, 8, 8, 8]              --\n",
       "├─AdaptiveAvgPool2d: 1-4                 [1, 8, 1, 1]              --\n",
       "├─Sequential: 1-5                        [1, 65]                   --\n",
       "│    └─Flatten: 2-11                     [1, 8]                    --\n",
       "│    └─Linear: 2-12                      [1, 65]                   585\n",
       "│    └─SiLU: 2-13                        [1, 65]                   --\n",
       "├─Linear: 1-6                            [1, 65]                   4,290\n",
       "==========================================================================================\n",
       "Total params: 5,323\n",
       "Trainable params: 5,323\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# アーキテクチャのテスト\n",
    "dqn = DQN()\n",
    "dummy_board = torch.zeros((1, 1, 8, 8))\n",
    "dummy_player = torch.ones((1, 1, 8, 8))  # 手番 +1\n",
    "dummy_input = torch.cat([dummy_board, dummy_player], dim=1)  # (1,2,8,8)\n",
    "print(dqn(dummy_input).shape)\n",
    "torchinfo.summary(dqn, (1, 2, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00283a17",
   "metadata": {},
   "source": [
    "## DQNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb3813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df06871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDoubleDQN:\n",
    "    \"\"\"Double DQN の学習クラス\n",
    "\n",
    "    Double DQN による Q 学習を実装します。経験再生、ターゲットネット更新、\n",
    "    ε-greedy 探索を備え、直近 N エピソードの平均報酬が改善したときに\n",
    "    ベストモデルを保持します。\n",
    "\n",
    "    Args:\n",
    "        dqn (nn.Module): Qネットワーク（入力: (B,2,8,8), 出力: (B,65)）\n",
    "        gamma (float): 割引率。Defaults to ``0.99``.\n",
    "        lr (float): 学習率。Defaults to ``1e-3``.\n",
    "        batch_size (int): バッチサイズ。Defaults to ``64``.\n",
    "        init_memory_size (int): リプレイ初期化のために集める遷移数。Defaults to ``5000``.\n",
    "        memory_size (int): リプレイバッファ容量。Defaults to ``50000``.\n",
    "        target_update_freq (int): ハードターゲット更新の頻度（更新回数）。Defaults to ``1000``.\n",
    "        tau (float): ソフト更新係数（>0 で Polyak）。``0.0`` ならハード更新。Defaults to ``0.0``.\n",
    "        num_episodes (int): 学習エピソード数。Defaults to ``1000``.\n",
    "        max_steps_per_episode (int): 1エピソードの最大ステップ。Defaults to ``120``.\n",
    "        train_freq (int): 何環境ステップごとに学習を走らせるか。Defaults to ``1``.\n",
    "        gradient_steps (int): 1回の学習トリガでの更新回数。Defaults to ``1``.\n",
    "        learning_starts (int): これ未満は学習を行わない。Defaults to ``1000``.\n",
    "        epsilon_start (float): ε-greedy 初期値。Defaults to ``1.0``.\n",
    "        epsilon_end (float): ε-greedy 最終値。Defaults to ``0.05``.\n",
    "        epsilon_decay_steps (int): ε を線形減衰させるステップ数。Defaults to ``30000``.\n",
    "        seed (int): 乱数シード。Defaults to ``42``.\n",
    "        device (torch.device): 使用デバイス。None なら自動選択。\n",
    "        ReplayBufferCls (type): リプレイバッファクラス（append/sample 実装必須）。\n",
    "        shaping_eta (float): ポテンシャルシェーピング係数 η。Defaults to ``0.1``.\n",
    "        rolling_window (int): ベスト判定のローリング平均窓幅。Defaults to ``100``.\n",
    "        save_best_path (str): ベスト更新ごとに保存するパス。None なら保存なし。\n",
    "\n",
    "    Attributes:\n",
    "        rewards (List[float]): 各エピソードの合計報酬ログ。\n",
    "        losses (List[float]): 各更新の損失ログ。\n",
    "        best_state_dict (Dict[str, torch.Tensor]): ベストモデルの重み。\n",
    "        best_score (float): ローリング平均のベストスコア。\n",
    "    \"\"\"\n",
    "\n",
    "    # =============================\n",
    "    # 初期化\n",
    "    # =============================\n",
    "    def __init__(\n",
    "        self,\n",
    "        dqn,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        init_memory_size: int = 5000,\n",
    "        memory_size: int = 50000,\n",
    "        target_update_freq: int = 1000,\n",
    "        tau: float = 0.0,\n",
    "        num_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 120,\n",
    "        train_freq: int = 1,\n",
    "        gradient_steps: int = 1,\n",
    "        learning_starts: int = 1000,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.05,\n",
    "        epsilon_decay_steps: int = 30000,\n",
    "        seed: int = 42,\n",
    "        device: Optional[torch.device] = None,\n",
    "        ReplayBufferCls=None,\n",
    "        shaping_eta: float = 0.1,\n",
    "        rolling_window: int = 100,\n",
    "        save_best_path: Optional[str] = None,\n",
    "    ):\n",
    "        assert ReplayBufferCls is not None, \"ReplayBufferCls を渡してください\"\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # 乱数シード固定（再現性）\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # オンライン/ターゲットネットの構築\n",
    "        self.dqn = dqn.to(self.device)\n",
    "        self.target_dqn = copy.deepcopy(dqn).to(self.device)\n",
    "        self.target_dqn.eval()\n",
    "\n",
    "        # 最適化器・損失関数\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.SmoothL1Loss()  # Huber\n",
    "\n",
    "        # ハイパーパラメータ類\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.init_memory_size = init_memory_size\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.num_episodes = num_episodes\n",
    "        self.train_freq = max(1, int(train_freq))\n",
    "        self.gradient_steps = max(1, int(gradient_steps))\n",
    "        self.learning_starts = int(learning_starts)\n",
    "\n",
    "        # ターゲット更新関連\n",
    "        self.tau = float(tau)\n",
    "        self.target_update_freq = int(target_update_freq)\n",
    "        self._num_updates = 0  # optimizer.step() 回数\n",
    "        self._num_env_steps = 0  # 環境ステップ数（ε 減衰にも使用）\n",
    "\n",
    "        # ε-greedy パラメータ\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = max(1, int(epsilon_decay_steps))\n",
    "\n",
    "        # 経験再生\n",
    "        self.replay_buffer = ReplayBufferCls(memory_size)\n",
    "\n",
    "        # shaping 係数\n",
    "        self.shaping_eta = float(shaping_eta)\n",
    "\n",
    "        # ログ/ベスト追跡\n",
    "        self.rewards: List[float] = []\n",
    "        self.losses: List[float] = []\n",
    "        self.rolling_window = int(rolling_window)\n",
    "        self.best_score = -float(\"inf\")\n",
    "        self.best_state_dict = copy.deepcopy(self.dqn.state_dict())\n",
    "        self.save_best_path = save_best_path\n",
    "\n",
    "        # ランダムプレイでリプレイを初期化\n",
    "        self._init_replay_buffer()\n",
    "\n",
    "    # =============================\n",
    "    # 公開 API\n",
    "    # =============================\n",
    "    def train(self, return_best_model: bool = False):\n",
    "        \"\"\"指定エピソード数だけ学習を実行し、必要ならベストモデルも返す。\n",
    "\n",
    "        エピソードを順に実行し、経験を蓄積・学習します。ローリング平均で\n",
    "        ベストが更新されたら状態辞書を保持し、オプションでファイルにも保存します。\n",
    "\n",
    "        Args:\n",
    "            return_best_model (bool, optional): True の場合、metrics とともに\n",
    "                ベストモデル（deepcopy 済み）を返します。\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[float]] | Tuple[Dict[str, List[float]], nn.Module]:\n",
    "                メトリクス辞書（rewards, losses）。return_best_model=True なら\n",
    "                これに加えてベストモデルを返します。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=self.num_episodes, desc=\"Train Double DQN\")\n",
    "        for ep in range(self.num_episodes):\n",
    "            # 1 エピソード実行\n",
    "            ep_reward = self._run_episode(ep)\n",
    "            self.rewards.append(ep_reward)\n",
    "\n",
    "            # 直近ローリング平均（窓幅に達するまでは全体平均）\n",
    "            if len(self.rewards) >= self.rolling_window:\n",
    "                rolling_avg = float(np.mean(self.rewards[-self.rolling_window:]))\n",
    "            else:\n",
    "                rolling_avg = float(np.mean(self.rewards))\n",
    "\n",
    "            # ベスト更新判定\n",
    "            if rolling_avg > self.best_score:\n",
    "                self.best_score = rolling_avg\n",
    "                self.best_state_dict = copy.deepcopy(self.dqn.state_dict())\n",
    "                if self.save_best_path is not None:\n",
    "                    torch.save(self.best_state_dict, self.save_best_path)\n",
    "\n",
    "            # 進捗バーの表示\n",
    "            last_loss = self.losses[-1] if self.losses else float(\"nan\")\n",
    "            pbar.set_postfix_str(\n",
    "                f\"Episode Reward: {ep_reward:.2f}  Roll@{self.rolling_window}: {rolling_avg:.2f}  \"\n",
    "                f\"Best: {self.best_score:.2f}  Loss: {last_loss:.3f}  ε: {self._epsilon_by_step(self._num_env_steps):.3f}\"\n",
    "            )\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        metrics = {\"rewards\": self.rewards, \"losses\": self.losses}\n",
    "        if return_best_model:\n",
    "            best_model = self._clone_model_with_state(self.best_state_dict)\n",
    "            return metrics, best_model\n",
    "        return metrics\n",
    "\n",
    "    def get_best_model(self) -> nn.Module:\n",
    "        \"\"\"保持しているベストの重みで初期化したモデルを返す。\n",
    "\n",
    "        Returns:\n",
    "            nn.Module: deepcopy 済みのモデルに best_state_dict を load して eval() 済み。\n",
    "        \"\"\"\n",
    "        return self._clone_model_with_state(self.best_state_dict)\n",
    "\n",
    "    # =============================\n",
    "    # 内部ヘルパ\n",
    "    # =============================\n",
    "    def _clone_model_with_state(self, state_dict) -> nn.Module:\n",
    "        \"\"\"オンラインネットのクローンを作成し、指定の重みを読み込む。\n",
    "\n",
    "        Args:\n",
    "            state_dict (Dict[str, torch.Tensor]): ロードする重み。\n",
    "\n",
    "        Returns:\n",
    "            nn.Module: device 上に配置され eval() 済みのコピー。\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(self.dqn).to(self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def _run_episode(self, episode_idx: int) -> float:\n",
    "        \"\"\"1 エピソード分の相互作用を実行して合計報酬を返す。\n",
    "\n",
    "        環境に対して ε-greedy で行動し、経験をリプレイバッファへ保存。\n",
    "        train_freq ごとに学習を走らせ、必要に応じてターゲット更新を行います。\n",
    "\n",
    "        Args:\n",
    "            episode_idx (int): エピソード番号（ログ/拡張用）。\n",
    "\n",
    "        Returns:\n",
    "            float: 当該エピソードの合計報酬。\n",
    "        \"\"\"\n",
    "        env = OthelloEnv()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done and steps < self.max_steps_per_episode:\n",
    "            # ε-greedy で行動選択\n",
    "            epsilon = self._epsilon_by_step(self._num_env_steps)\n",
    "            player = env.player  # 現在手番（+1/-1）\n",
    "            state = env.get_state()  # (8,8) float32\n",
    "            action = self._select_action_by_epsilon_greedy(env, epsilon)\n",
    "\n",
    "            # シェーピング報酬：prev/next から r を算出\n",
    "            shaped = ShapedReward(player, eta=self.shaping_eta, gamma=self.gamma)\n",
    "            next_state, reward, done, _ = env.step(action, reward_fn=shaped.get_reward)\n",
    "            next_player = env.player\n",
    "\n",
    "            # 次状態の合法手とともに遷移を保存\n",
    "            next_legal_actions = self._legal_actions_from_board(next_state, next_player)\n",
    "            self._store_transition(\n",
    "                board=state,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                next_board=next_state,\n",
    "                done=done,\n",
    "                player=player,\n",
    "                next_player=next_player,\n",
    "                next_legal_actions=next_legal_actions,\n",
    "            )\n",
    "\n",
    "            # バッファが暖まっていれば学習を実行\n",
    "            if (self._num_env_steps % self.train_freq == 0) and (len(self.replay_buffer) >= max(self.batch_size, self.learning_starts)):\n",
    "                for _ in range(self.gradient_steps):\n",
    "                    loss = self._update_dqn_double()\n",
    "                    if not math.isnan(loss):\n",
    "                        self.losses.append(loss)\n",
    "\n",
    "            # 必要ならターゲット更新\n",
    "            self._maybe_update_target()\n",
    "\n",
    "            total_reward += float(reward)\n",
    "            steps += 1\n",
    "            self._num_env_steps += 1\n",
    "\n",
    "        return float(total_reward)\n",
    "\n",
    "    def _update_dqn_double(self) -> float:\n",
    "        \"\"\"Double DQN の 1 回分の更新を実行して損失を返す。\n",
    "\n",
    "        next 状態でオンラインネットにより argmax を選び、ターゲットネットで\n",
    "        その値を評価（Double DQN）。かつ、合法手のみを有効化するマスクを適用します。\n",
    "\n",
    "        Returns:\n",
    "            float: Huber 損失のスカラー値。\n",
    "        \"\"\"\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        boards_np = np.stack([b['board'] for b in batch])              # (B,8,8)\n",
    "        next_boards_np = np.stack([b['next_board'] for b in batch])    # (B,8,8)\n",
    "        players_np = np.array([b['player'] for b in batch], dtype=np.float32)\n",
    "        next_players_np = np.array([b['next_player'] for b in batch], dtype=np.float32)\n",
    "\n",
    "        board = self._batch_to_input(boards_np, players_np)            # (B,2,8,8)\n",
    "        next_board = self._batch_to_input(next_boards_np, next_players_np)\n",
    "\n",
    "        # (B,2,8,8) テンソルを構築（チャネル0=盤面, チャネル1=手番）\n",
    "        board = torch.stack([self._to_input(b['board'], b['player']) for b in batch]).to(self.device)\n",
    "        next_board = torch.stack([self._to_input(b['next_board'], b['next_player']) for b in batch]).to(self.device)\n",
    "\n",
    "        # スカラー群\n",
    "        action = torch.tensor([b['action'] for b in batch], dtype=torch.int64, device=self.device)\n",
    "        reward = torch.tensor([b['reward'] for b in batch], dtype=torch.float32, device=self.device)\n",
    "        done = torch.tensor([b['done'] for b in batch], dtype=torch.float32, device=self.device)\n",
    "        next_legal_actions_list = [b['next_legal_actions'] for b in batch]\n",
    "\n",
    "        # Q(s,a) を抽出\n",
    "        q_all = self.dqn(board)                                 # (B,65)\n",
    "        q_sa = q_all.gather(1, action.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 次状態で非合法手をマスク（-1e9 加算）\n",
    "            next_masks = self._build_masks_from_indices(next_legal_actions_list, fill_value=-1e9)\n",
    "            q_next_online = self.dqn(next_board) + next_masks\n",
    "            next_actions_online = q_next_online.argmax(dim=1)  # (B,)\n",
    "\n",
    "            # ターゲットネットで評価\n",
    "            q_next_target = self.target_dqn(next_board)\n",
    "            next_q = q_next_target.gather(1, next_actions_online.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "\n",
    "            # ベルマンターゲット\n",
    "            target = reward + self.gamma * next_q * (1.0 - done)\n",
    "\n",
    "        loss = self.loss_fn(q_sa, target)\n",
    "        if torch.isnan(loss):  # まれな数値異常を検知\n",
    "            return float(\"nan\")\n",
    "\n",
    "        # 逆伝播と最適化\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dqn.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self._num_updates += 1\n",
    "        return float(loss.item())\n",
    "\n",
    "    def _select_action_by_epsilon_greedy(self, env, epsilon: float) -> int:\n",
    "        \"\"\"ε-greedy に基づき合法手の中から行動をサンプルする。\n",
    "\n",
    "        Args:\n",
    "            env (OthelloEnv): 環境インスタンス。\n",
    "            epsilon (float): 探索率 ε（0〜1）。\n",
    "\n",
    "        Returns:\n",
    "            int: 選択した行動インデックス（0..64）。\n",
    "        \"\"\"\n",
    "        legal_actions = env.legal_actions()\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(legal_actions)\n",
    "        return self._select_action_by_greedy(env)\n",
    "\n",
    "    def _select_action_by_greedy(self, env) -> int:\n",
    "        \"\"\"合法手のみを対象に Q を比較し、貪欲（argmax）に行動を選ぶ。\n",
    "\n",
    "        Args:\n",
    "            env (OthelloEnv): 環境インスタンス。\n",
    "\n",
    "        Returns:\n",
    "            int: 貪欲に選んだ行動インデックス（0..64）。\n",
    "        \"\"\"\n",
    "        legal_actions = env.legal_actions()\n",
    "        board_tensor = self._to_input(env.get_state(), env.player).unsqueeze(0).to(self.device)  # (1,2,8,8)\n",
    "        with torch.no_grad():\n",
    "            q_all = self.dqn(board_tensor).squeeze(0)  # (65,)\n",
    "            # 非合法手を巨大負値で潰す\n",
    "            mask = torch.full((65,), -1e9, device=self.device)\n",
    "            for a in legal_actions:\n",
    "                mask[a] = 0.0\n",
    "            q_masked = q_all + mask\n",
    "            action = int(q_masked.argmax().item())\n",
    "        return action\n",
    "\n",
    "    def _init_replay_buffer(self):\n",
    "        \"\"\"ランダムポリシーで遷移を収集し、リプレイバッファを初期化する。\"\"\"\n",
    "        target = min(self.init_memory_size, self.replay_buffer.memory_size)\n",
    "        added = 0\n",
    "        pbar = tqdm(total=target, desc='Init replay buffer')\n",
    "        while added < target:\n",
    "            env = OthelloEnv()\n",
    "            done = False\n",
    "            while not done and added < target:\n",
    "                player = env.player\n",
    "                state = env.get_state()\n",
    "                legal_actions = env.legal_actions()\n",
    "                action = random.choice(legal_actions)\n",
    "                shaped = ShapedReward(player, eta=self.shaping_eta, gamma=self.gamma)\n",
    "                next_state, reward, done, _ = env.step(action, reward_fn=shaped.get_reward)\n",
    "                next_player = env.player\n",
    "                next_legal_actions = self._legal_actions_from_board(next_state, next_player)\n",
    "\n",
    "                self._store_transition(\n",
    "                    board=state,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    next_board=next_state,\n",
    "                    done=done,\n",
    "                    player=player,\n",
    "                    next_player=next_player,\n",
    "                    next_legal_actions=next_legal_actions,\n",
    "                )\n",
    "                added += 1\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    def _maybe_update_target(self):\n",
    "        \"\"\"ターゲットネットを Polyak またはハードコピーで更新する。\n",
    "\n",
    "        * tau > 0 の場合: ``target = (1-tau)*target + tau*online``（ソフト更新）\n",
    "        * それ以外: optimizer の更新回数で target_update_freq ごとにハード更新\n",
    "        \"\"\"\n",
    "        if self.tau and self.tau > 0.0:\n",
    "            with torch.no_grad():\n",
    "                for tp, p in zip(self.target_dqn.parameters(), self.dqn.parameters()):\n",
    "                    tp.data.mul_(1.0 - self.tau).add_(self.tau * p.data)\n",
    "        else:\n",
    "            if (self._num_updates % max(1, self.target_update_freq)) == 0 and self._num_updates > 0:\n",
    "                self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _epsilon_by_step(self, step: int) -> float:\n",
    "        \"\"\"環境ステップ数に応じて ε を線形減衰させる。\n",
    "\n",
    "        Args:\n",
    "            step (int): 現在の環境ステップ数。\n",
    "\n",
    "        Returns:\n",
    "            float: [epsilon_end, epsilon_start] の範囲での ε 値。\n",
    "        \"\"\"\n",
    "        if step >= self.epsilon_decay_steps:\n",
    "            return self.epsilon_end\n",
    "        span = self.epsilon_start - self.epsilon_end\n",
    "        return self.epsilon_start - span * (step / self.epsilon_decay_steps)\n",
    "\n",
    "    def _build_masks_from_indices(self, batch_next_legal_actions, fill_value: float = -1e9):\n",
    "        \"\"\"次状態ごとの合法手リストから加算マスクを作る。\n",
    "\n",
    "        Args:\n",
    "            batch_next_legal_actions (List[List[int]]): 各バッチにおける合法手のリスト。\n",
    "            fill_value (float): 非合法手に与える値（例: -1e9）。Defaults to ``-1e9``.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 形状 (B,65) のマスクテンソル（合法手は 0、非合法手は fill_value）。\n",
    "        \"\"\"\n",
    "        B = len(batch_next_legal_actions)\n",
    "        masks = torch.full((B, 65), float(fill_value), device=self.device)\n",
    "        for i, acts in enumerate(batch_next_legal_actions):\n",
    "            for a in acts:\n",
    "                masks[i, a] = 0.0\n",
    "        return masks\n",
    "\n",
    "    def _to_input(self, board_like, player_scalar: int) -> torch.Tensor:\n",
    "        \"\"\"盤面と手番スカラーから 2 チャンネルテンソルを生成する。\n",
    "\n",
    "        チャネル0: 盤面（-1,0,+1）。チャネル1: 手番プレーン（+1/-1 で全面を塗る）。\n",
    "\n",
    "        Args:\n",
    "            board_like (array-like): 形状 (8,8) または (1,8,8) の配列。\n",
    "            player_scalar (int): +1（黒手番）または -1（白手番）。\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 形状 (2,8,8) の float32 テンソル。\n",
    "        \"\"\"\n",
    "        t = torch.as_tensor(board_like, dtype=torch.float32)\n",
    "        if t.dim() == 2 and t.shape == (8, 8):\n",
    "            t = t.unsqueeze(0)  # (1,8,8)\n",
    "        elif t.dim() == 3 and t.shape == (1, 8, 8):\n",
    "            pass\n",
    "        else:\n",
    "            t = t.reshape(1, 8, 8)\n",
    "        player_plane = torch.full_like(t, float(player_scalar))  # (1,8,8)\n",
    "        return torch.cat([t, player_plane], dim=0)  # (2,8,8)\n",
    "\n",
    "    def _batch_to_input(self, boards_np: np.ndarray, players_np: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        boards_np: (B, 8, 8), players_np: (B,)\n",
    "        \"\"\"\n",
    "        t = torch.as_tensor(boards_np, dtype=torch.float32, device=self.device).unsqueeze(1)  # (B,1,8,8)\n",
    "        p = torch.as_tensor(players_np, dtype=torch.float32, device=self.device).view(-1,1,1,1).expand(-1,1,8,8)\n",
    "        return torch.cat([t, p], dim=1)  # (B,2,8,8)\n",
    "\n",
    "    def _legal_actions_from_board(self, board_np: np.ndarray, player: int) -> List[int]:\n",
    "        \"\"\"次状態の盤面と手番から合法手（インデックス配列）を再計算する。\n",
    "\n",
    "        Args:\n",
    "            board_np (np.ndarray): 形状 (8,8) の盤面（np.float32 互換）。\n",
    "            player (int): +1（黒）または -1（白）。\n",
    "\n",
    "        Returns:\n",
    "            List[int]: 合法手のインデックス。合法手が無ければ [64]（パス）。\n",
    "        \"\"\"\n",
    "        # ここでは副作用を避けるため、OthelloGame を新規に作り直す\n",
    "        g = OthelloEnv().game.__class__()  # 既存環境からゲームクラスを取得して生成\n",
    "        g.board = board_np.astype(np.int8).tolist()\n",
    "        g.player = player\n",
    "        moves = g.legal_moves(player)\n",
    "        if not moves:\n",
    "            return [64]\n",
    "        return [r * 8 + c for r, c in moves]\n",
    "\n",
    "    def _store_transition(\n",
    "        self,\n",
    "        board,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_board,\n",
    "        done: bool,\n",
    "        player: int,\n",
    "        next_player: int,\n",
    "        next_legal_actions: List[int],\n",
    "    ):\n",
    "        \"\"\"1 ステップ分の遷移をリプレイバッファに保存する。\n",
    "\n",
    "        Args:\n",
    "            board (np.ndarray): 現在の盤面 (8,8)。\n",
    "            action (int): 実行した行動インデックス。\n",
    "            reward (float): 即時報酬。\n",
    "            next_board (np.ndarray): 次の盤面 (8,8)。\n",
    "            done (bool): 終局フラグ。\n",
    "            player (int): 現在盤面での手番（+1/-1）。\n",
    "            next_player (int): 次盤面での手番（+1/-1）。\n",
    "            next_legal_actions (List[int]): 次状態での合法手インデックス一覧。\n",
    "        \"\"\"\n",
    "        transition = {\n",
    "            \"board\": np.array(board, dtype=np.float32),\n",
    "            \"action\": int(action),\n",
    "            \"reward\": float(reward),\n",
    "            \"next_board\": np.array(next_board, dtype=np.float32),\n",
    "            \"done\": bool(done),\n",
    "            \"player\": int(player),\n",
    "            \"next_player\": int(next_player),\n",
    "            \"next_legal_actions\": next_legal_actions,\n",
    "        }\n",
    "        self.replay_buffer.append(transition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52fdaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7d04edfe5a4e20bb6f3820896f8a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Init replay buffer:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da321cbb5a2d435bab5761a4c25f72c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Double DQN:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to model_weights/ddqn/2025-09-21_12-47-29.pth\n",
      "Best model loaded into trainer.dqn\n"
     ]
    }
   ],
   "source": [
    "# デバイス自動選択\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# DQN を作成（あなたの2ch対応版）\n",
    "dqn = DQN().to(device)\n",
    "\n",
    "# リプレイバッファ\n",
    "replay_cls = ReplayBuffer\n",
    "\n",
    "# ハイパラ（まずは軽めに）\n",
    "trainer = TrainDoubleDQN(\n",
    "    dqn=dqn,\n",
    "    device=device,\n",
    "    ReplayBufferCls=replay_cls,\n",
    "    num_episodes=10,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    target_update_freq=200,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay_steps=1000,\n",
    "    max_steps_per_episode=120,\n",
    "    shaping_eta=0.1,\n",
    "    rolling_window=100,\n",
    "    save_best_path=\"best_dqn.pth\",  # ベスト更新のたび保存（不要なら None）\n",
    ")\n",
    "\n",
    "# 学習開始：ベストモデルも受け取る\n",
    "metrics, best_model = trainer.train(return_best_model=True)\n",
    "\n",
    "# 学習曲線を確認\n",
    "plt.figure()\n",
    "plt.plot(metrics[\"rewards\"])\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "# ベストモデルの重みを保存\n",
    "# ファイル名は、タイムスタンプを含める\n",
    "save_dir = \"model_weights/ddqn\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_weights_path = f\"{save_dir}/{timestamp}.pth\"\n",
    "torch.save(best_model.state_dict(), save_weights_path)\n",
    "print(f\"Best model saved to {save_weights_path}\")\n",
    "\n",
    "# ベスト重みでオンラインネットを上書き（以降の推論に反映したい場合）\n",
    "trainer.dqn.load_state_dict(best_model.state_dict())\n",
    "trainer.dqn.eval()\n",
    "print(\"Best model loaded into trainer.dqn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
