{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de3e60d",
   "metadata": {},
   "source": [
    "## DQNでエージェントを構築\n",
    "- Othelloの報酬設計と、環境は、utilsディレクトリ内に保存している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef50696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import datetime\n",
    "import inspect\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import torchinfo\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# ローカル環境/報酬\n",
    "from utils.mori.othello_env import OthelloEnv\n",
    "from utils.mori.othello_game import OthelloGame\n",
    "from utils.mori.othello_reward import ShapedReward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af76fb",
   "metadata": {},
   "source": [
    "## DQNのネットワーク定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba46330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch: int, bn_eps: float = 1e-5, zero_init: bool = True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(ch, eps=bn_eps)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(ch, eps=bn_eps)\n",
    "        if zero_init:\n",
    "            # 出力を初期は恒等写像に近づけて安定化\n",
    "            nn.init.zeros_(self.bn2.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.relu(self.bn1(self.conv1(x)))\n",
    "        h = self.bn2(self.conv2(h))\n",
    "        return F.relu(x + h)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int = 2,\n",
    "        width: int = 32,\n",
    "        num_res_blocks: int = 3,\n",
    "        bn_eps: float = 1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, width, 3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, width, eps=bn_eps),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResBlock(width, bn_eps=bn_eps, zero_init=False) for _ in range(num_res_blocks)]\n",
    "        )\n",
    "        # 65アクション（0..63: 盤上, 64: パス）\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=width, out_channels=2, kernel_size=1, bias=False),  # (B,2,8,8)\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.logits_fc = nn.Linear(2 * 8 * 8, 65)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.stem(x)\n",
    "        h = self.res_blocks(h)\n",
    "        h = self.policy_head(h)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        logits = self.logits_fc(h)  # (B,65)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc003cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DQN                                      [1, 65]                   --\n",
       "├─Sequential: 1-1                        [1, 32, 8, 8]             --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 8, 8]             576\n",
       "│    └─GroupNorm: 2-2                    [1, 32, 8, 8]             64\n",
       "│    └─ReLU: 2-3                         [1, 32, 8, 8]             --\n",
       "├─Sequential: 1-2                        [1, 32, 8, 8]             --\n",
       "│    └─ResBlock: 2-4                     [1, 32, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-1                  [1, 32, 8, 8]             9,216\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 32, 8, 8]             64\n",
       "│    │    └─Conv2d: 3-3                  [1, 32, 8, 8]             9,216\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 32, 8, 8]             64\n",
       "│    └─ResBlock: 2-5                     [1, 32, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-5                  [1, 32, 8, 8]             9,216\n",
       "│    │    └─BatchNorm2d: 3-6             [1, 32, 8, 8]             64\n",
       "│    │    └─Conv2d: 3-7                  [1, 32, 8, 8]             9,216\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 32, 8, 8]             64\n",
       "│    └─ResBlock: 2-6                     [1, 32, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-9                  [1, 32, 8, 8]             9,216\n",
       "│    │    └─BatchNorm2d: 3-10            [1, 32, 8, 8]             64\n",
       "│    │    └─Conv2d: 3-11                 [1, 32, 8, 8]             9,216\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 32, 8, 8]             64\n",
       "├─Sequential: 1-3                        [1, 2, 8, 8]              --\n",
       "│    └─Conv2d: 2-7                       [1, 2, 8, 8]              64\n",
       "│    └─ReLU: 2-8                         [1, 2, 8, 8]              --\n",
       "├─Linear: 1-4                            [1, 65]                   8,385\n",
       "==========================================================================================\n",
       "Total params: 64,769\n",
       "Trainable params: 64,769\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 3.59\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.23\n",
       "Params size (MB): 0.26\n",
       "Estimated Total Size (MB): 0.49\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# アーキテクチャのテスト\n",
    "dqn = DQN()\n",
    "dummy_board = torch.zeros((1, 1, 8, 8))\n",
    "dummy_player = torch.ones((1, 1, 8, 8))  # 手番 +1\n",
    "dummy_input = torch.cat([dummy_board, dummy_player], dim=1)  # (1,2,8,8)\n",
    "print(dqn(dummy_input).shape)\n",
    "torchinfo.summary(dqn, (1, 2, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff81440",
   "metadata": {},
   "source": [
    "## 対戦相手用の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3d72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    \"\"\" 盤面が入力されたらその評価値(-1~1)を出力するネットワーク \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int = 2,\n",
    "        width: int = 32,\n",
    "        num_res_blocks: int = 3,\n",
    "        bn_eps: float = 1e-5,\n",
    "        head_hidden_size: int = 32,\n",
    "        use_gap: bool = True,\n",
    "        norm_head: str = \"ln\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, width, 3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, width, eps=bn_eps),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResBlock(width, bn_eps=bn_eps, zero_init=True) for _ in range(num_res_blocks)]\n",
    "        )\n",
    "\n",
    "        # value head: 1x1 conv → (norm) → ReLU\n",
    "        self.value_conv = nn.Conv2d(width, 1, kernel_size=1, bias=False)\n",
    "\n",
    "        if norm_head == \"bn\":\n",
    "            self.value_norm = nn.GroupNorm(1, 1, eps=bn_eps)\n",
    "        elif norm_head == \"ln\":\n",
    "            # LayerNorm over (C,H,W) = (1,8,8) -> normalized_shape=(1,8,8)\n",
    "            self.value_norm = nn.LayerNorm((1, 8, 8))\n",
    "        elif norm_head == \"gn\":\n",
    "            self.value_norm = nn.GroupNorm(1, 1)  # 1 group = LayerNorm的\n",
    "        else:\n",
    "            self.value_norm = nn.Identity()\n",
    "\n",
    "        self.use_gap = use_gap\n",
    "        if use_gap:\n",
    "            in_fc = 1  # GAPで(1,)に\n",
    "        else:\n",
    "            in_fc = 8 * 8\n",
    "\n",
    "        self.value_fc1 = nn.Linear(in_fc, head_hidden_size)\n",
    "        self.value_fc2 = nn.Linear(head_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.stem(x)\n",
    "        h = self.res_blocks(h)\n",
    "\n",
    "        h = self.value_conv(h)  # (B,1,8,8)\n",
    "        if isinstance(self.value_norm, nn.LayerNorm):\n",
    "            h = self.value_norm(h)  # 形状そのまま\n",
    "        else:\n",
    "            h = self.value_norm(h)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        if self.use_gap:\n",
    "            h = h.mean(dim=(2, 3), keepdim=False)  # (B,1)\n",
    "        else:\n",
    "            h = h.view(h.size(0), -1)             # (B,64)\n",
    "\n",
    "        h = F.relu(self.value_fc1(h))\n",
    "        v = torch.tanh(self.value_fc2(h))         # [-1, 1]\n",
    "        return v  # (B,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00283a17",
   "metadata": {},
   "source": [
    "## DQNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb3813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df06871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDoubleDQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dqn: nn.Module,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        init_memory_size: int = 5000,\n",
    "        memory_size: int = 50000,\n",
    "        target_update_freq: int = 1000,\n",
    "        tau: float = 0.005,\n",
    "        num_episodes: int = 1000,\n",
    "        max_games_per_episode: int = 5,\n",
    "        train_freq: int = 1,\n",
    "        gradient_steps: int = 1,\n",
    "        learning_starts: int = 1000,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.05,\n",
    "        epsilon_decay_steps: int = 30000,\n",
    "        seed: int = 42,\n",
    "        device: Optional[torch.device] = None,\n",
    "        ReplayBufferCls=ReplayBuffer,\n",
    "        rolling_window: int = 100,\n",
    "        save_best_path: Optional[str] = None,\n",
    "        pretrained_opp_path: Optional[str] = None,\n",
    "        opponent_cycle: Optional[Tuple[str]] = None,\n",
    "        progress_callback = None,\n",
    "    ):\n",
    "        assert ReplayBufferCls is not None, \"ReplayBufferCls を渡してください\"\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "        # 再現性\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # ネットワーク\n",
    "        self.dqn = dqn.to(self.device)\n",
    "        self.target_dqn = copy.deepcopy(dqn).to(self.device)\n",
    "        self.target_dqn.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # 相手のモデル\n",
    "        self.opp_model = None\n",
    "        if pretrained_opp_path is not None:\n",
    "            self.opp_model = CriticNet().to(self.device)\n",
    "            self.opp_model.load_state_dict(torch.load(pretrained_opp_path))\n",
    "            self.opp_model.eval()\n",
    "\n",
    "        # HP\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.init_memory_size = int(init_memory_size)\n",
    "        self.max_games_per_episode = int(max_games_per_episode)\n",
    "        self.num_episodes = int(num_episodes)\n",
    "        self.train_freq = max(1, int(train_freq))\n",
    "        self.gradient_steps = max(1, int(gradient_steps))\n",
    "        self.learning_starts = int(learning_starts)\n",
    "\n",
    "        self.tau = float(tau)\n",
    "        self.target_update_freq = int(target_update_freq)\n",
    "        self._num_updates = 0\n",
    "        self._num_env_steps = 0\n",
    "\n",
    "        self.epsilon_start = float(epsilon_start)\n",
    "        self.epsilon_end = float(epsilon_end)\n",
    "        self.epsilon_decay_steps = max(1, int(epsilon_decay_steps))\n",
    "\n",
    "        self.replay_buffer = ReplayBufferCls(int(memory_size))\n",
    "\n",
    "        self.rewards: List[float] = []\n",
    "        self.losses: List[float] = []\n",
    "        self.rolling_window = int(rolling_window)\n",
    "        self.best_score = -float(\"inf\")\n",
    "        self.best_state_dict = copy.deepcopy(self.dqn.state_dict())\n",
    "        self.save_best_path = save_best_path\n",
    "        self.progress_callback = progress_callback\n",
    "\n",
    "        # 対戦モード管理\n",
    "        default_cycle = [\"self\", \"random\"]\n",
    "        if self.opp_model is not None:\n",
    "            default_cycle.append(\"critic\")\n",
    "\n",
    "        if opponent_cycle is None:\n",
    "            opponent_cycle = tuple(default_cycle)\n",
    "        else:\n",
    "            validated = []\n",
    "            for mode in opponent_cycle:\n",
    "                if mode == \"critic\" and self.opp_model is None:\n",
    "                    raise ValueError(\"critic を指定していますが pretrained_opp_path がありません。\")\n",
    "                if mode not in (\"self\", \"random\", \"critic\"):\n",
    "                    raise ValueError(f\"未知の opponent '{mode}'\")\n",
    "                validated.append(mode)\n",
    "            if not validated:\n",
    "                raise ValueError(\"opponent_cycle が空です。\")\n",
    "            opponent_cycle = tuple(validated)\n",
    "\n",
    "        self.opponent_cycle = opponent_cycle\n",
    "        self.opponent_reward_logs: Dict[str, List[float]] = {m: [] for m in self.opponent_cycle}\n",
    "        self.latest_opponent_reward: Dict[str, float] = {m: float(\"nan\") for m in self.opponent_cycle}\n",
    "\n",
    "        # 初期リプレイ収集\n",
    "        self._init_replay_buffer()\n",
    "\n",
    "    # --------- 公開 API ---------\n",
    "    def train(self, return_best_model: bool = False):\n",
    "        pbar = tqdm(total=self.num_episodes, desc=\"Train Double DQN\")\n",
    "        for ep in range(self.num_episodes):\n",
    "            opponent = self.opponent_cycle[ep % len(self.opponent_cycle)]\n",
    "\n",
    "            if opponent == \"self\":\n",
    "                ep_reward = self._run_episodes_with_self(ep)\n",
    "            elif opponent == \"random\":\n",
    "                ep_reward = self._run_episode_with_random(ep)\n",
    "            elif opponent == \"critic\":\n",
    "                ep_reward = self._run_episode_with_pretrained_critic(ep)\n",
    "            else:\n",
    "                raise RuntimeError(f\"未知の opponent '{opponent}'\")\n",
    "\n",
    "            self.rewards.append(ep_reward)\n",
    "            self.opponent_reward_logs[opponent].append(ep_reward)\n",
    "            self.latest_opponent_reward[opponent] = ep_reward\n",
    "\n",
    "            if len(self.rewards) >= self.rolling_window:\n",
    "                rolling_avg = float(np.mean(self.rewards[-self.rolling_window:]))\n",
    "            else:\n",
    "                rolling_avg = float(np.mean(self.rewards))\n",
    "\n",
    "            if rolling_avg > self.best_score:\n",
    "                self.best_score = rolling_avg\n",
    "                self.best_state_dict = copy.deepcopy(self.dqn.state_dict())\n",
    "                if self.save_best_path is not None:\n",
    "                    torch.save(self.best_state_dict, self.save_best_path)\n",
    "\n",
    "            last_loss = self.losses[-1] if self.losses else float(\"nan\")\n",
    "            pbar.set_postfix_str(\n",
    "                f\"EpR {ep_reward*64:.2f}, Roll@{self.rolling_window}: {rolling_avg*64:.2f}, Best: {self.best_score:.2f}, Loss: {last_loss:.3f}, ε: {self._epsilon_by_step(self._num_env_steps):.3f}\"\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "            if self.progress_callback is not None:\n",
    "                info = {\n",
    "                    \"opponent\": opponent,\n",
    "                    \"latest_rewards\": {\n",
    "                        mode: (logs[-1] if logs else float(\"nan\"))\n",
    "                        for mode, logs in self.opponent_reward_logs.items()\n",
    "                    },\n",
    "                    \"history\": {mode: list(logs) for mode, logs in self.opponent_reward_logs.items()},\n",
    "                }\n",
    "                params = inspect.signature(self.progress_callback).parameters\n",
    "                if len(params) <= 4:\n",
    "                    self.progress_callback(ep, ep_reward, rolling_avg, last_loss)\n",
    "                else:\n",
    "                    self.progress_callback(ep, ep_reward, rolling_avg, last_loss, info)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        metrics = {\"rewards\": self.rewards, \"losses\": self.losses}\n",
    "        if return_best_model:\n",
    "            best_model = self._clone_model_with_state(self.best_state_dict)\n",
    "            return metrics, best_model\n",
    "        return metrics\n",
    "\n",
    "    def get_best_model(self) -> nn.Module:\n",
    "        \"\"\"ベストモデルを返す\"\"\"\n",
    "        return self._clone_model_with_state(self.best_state_dict)\n",
    "\n",
    "    def _clone_model_with_state(self, state_dict) -> nn.Module:\n",
    "        \"\"\"モデルを複製して、指定された状態を設定する\"\"\"\n",
    "        model = copy.deepcopy(self.dqn).to(self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def _run_episodes_with_self(self, episode_idx: int) -> float:\n",
    "        \"\"\" エピソードを複数回実行する \"\"\"\n",
    "        env = OthelloEnv()\n",
    "        # 手番をランダム化（約 50% で白番スタートに切り替え）\n",
    "        if random.random() < 0.5:\n",
    "            env.step(64)\n",
    "        total_reward = 0.0\n",
    "        games = 0\n",
    "        done = False\n",
    "\n",
    "        while not done and games < self.max_games_per_episode:\n",
    "            # ε-greedy 選択\n",
    "            epsilon = self._epsilon_by_step(self._num_env_steps)\n",
    "            player = env.player\n",
    "            state = env.get_state()\n",
    "            action = self._select_action_by_epsilon_greedy(env, epsilon)\n",
    "\n",
    "            # シェーピング付き報酬\n",
    "            reward_fn = ShapedReward(player)\n",
    "            next_state, reward, done, _ = env.step(action, reward_fn=reward_fn)\n",
    "            next_player = env.player\n",
    "\n",
    "            next_legal_actions = self._legal_actions_from_board(next_state[0], next_player)\n",
    "\n",
    "            # 状態遷移を保存\n",
    "            # 終局の場合は３回状態遷移を保存（通常時は1回）\n",
    "            if done:\n",
    "                num_save = 3\n",
    "            else:\n",
    "                num_save = 1\n",
    "            for _ in range(num_save):\n",
    "                self._store_transition(\n",
    "                    board=state[0],\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    next_board=next_state[0],\n",
    "                    done=done,\n",
    "                    player=player,\n",
    "                    next_player=next_player,\n",
    "                    next_legal_actions=next_legal_actions,\n",
    "                )\n",
    "            self._num_env_steps += 1\n",
    "\n",
    "            # 学習トリガ\n",
    "            if (self._num_env_steps % self.train_freq == 0) and (len(self.replay_buffer) >= max(self.batch_size, self.learning_starts)):\n",
    "                for _ in range(self.gradient_steps):\n",
    "                    loss = self._update_dqn_double()\n",
    "                    if not math.isnan(loss):\n",
    "                        self.losses.append(loss)\n",
    "\n",
    "            # ターゲット更新\n",
    "            self._maybe_update_target()\n",
    "\n",
    "            total_reward += float(reward)\n",
    "\n",
    "            if done:\n",
    "                games += 1\n",
    "\n",
    "        return float(total_reward)\n",
    "\n",
    "    def _run_episode_with_random(self, episode_idx: int) -> float:\n",
    "        \"\"\"ランダム相手と対戦しながら 1 〜 max_games_per_episode 局プレイして学習する\"\"\"\n",
    "        total_reward = 0.0\n",
    "        games = 0\n",
    "\n",
    "        while games < self.max_games_per_episode:\n",
    "            env = OthelloEnv()\n",
    "\n",
    "            # 開始手番をランダム化（約 50% で白番スタートに切り替え）\n",
    "            if np.random.rand() < 0.5:\n",
    "                env.step(64)  # パスで手番を反転\n",
    "            my_color = env.player  # 現在の手番が自分の色\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                player = env.player\n",
    "                state = env.get_state()\n",
    "\n",
    "                # 自分の手番の場合のみε-greedy\n",
    "                if player == my_color:\n",
    "                    epsilon = self._epsilon_by_step(self._num_env_steps)\n",
    "                    action = self._select_action_by_epsilon_greedy(env, epsilon)\n",
    "                else:\n",
    "                    action = random.choice(env.legal_actions())\n",
    "\n",
    "                # 自分視点の報酬関数\n",
    "                reward_fn = ShapedReward(my_color)\n",
    "                next_state, reward, done, _ = env.step(action, reward_fn=reward_fn)\n",
    "                next_player = env.player\n",
    "                next_legal_actions = self._legal_actions_from_board(next_state[0], next_player)\n",
    "\n",
    "                # 自分の手番の場合のみ状態遷移を保存\n",
    "                if player == my_color:\n",
    "                    # 状態遷移を保存\n",
    "                    # 終局の場合は３回状態遷移を保存（通常時は1回）\n",
    "                    if done:\n",
    "                        num_save = 3\n",
    "                    else:\n",
    "                        num_save = 1\n",
    "                    for _ in range(num_save):\n",
    "                        self._store_transition(\n",
    "                            board=state[0],\n",
    "                            action=action,\n",
    "                            reward=reward,\n",
    "                            next_board=next_state[0],\n",
    "                            done=done,\n",
    "                            player=player,\n",
    "                            next_player=next_player,\n",
    "                            next_legal_actions=next_legal_actions,\n",
    "                        )\n",
    "                    self._num_env_steps += 1\n",
    "\n",
    "                # 学習トリガ\n",
    "                if (\n",
    "                    (self._num_env_steps % self.train_freq == 0)\n",
    "                    and (len(self.replay_buffer) >= max(self.batch_size, self.learning_starts))\n",
    "                ):\n",
    "                    for _ in range(self.gradient_steps):\n",
    "                        loss = self._update_dqn_double()\n",
    "                        if not math.isnan(loss):\n",
    "                            self.losses.append(loss)\n",
    "\n",
    "                # ターゲット更新\n",
    "                self._maybe_update_target()\n",
    "\n",
    "                total_reward += float(reward)\n",
    "\n",
    "            games += 1\n",
    "\n",
    "        return float(total_reward)\n",
    "\n",
    "    def _run_episode_with_pretrained_critic(\n",
    "        self,\n",
    "        episode_idx: int,\n",
    "    ) -> float:\n",
    "        \"\"\"事前学習済み Critic を相手に複数局プレイして学習する\"\"\"\n",
    "        if not hasattr(self, \"opp_model\") or self.opp_model is None:\n",
    "            raise ValueError(\"pretrained critic がロードされていません。\")\n",
    "\n",
    "        self.opp_model.eval()\n",
    "\n",
    "        total_reward = 0.0\n",
    "        games = 0\n",
    "\n",
    "        while games < self.max_games_per_episode:\n",
    "            env = OthelloEnv()\n",
    "\n",
    "            # 50% の確率で白番スタートにして多様性を確保\n",
    "            if random.random() < 0.5:\n",
    "                env.step(64)\n",
    "            my_color = env.player\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                player = env.player\n",
    "                state = env.get_state()\n",
    "\n",
    "                if player == my_color:\n",
    "                    epsilon = self._epsilon_by_step(self._num_env_steps)\n",
    "                    action = self._select_action_by_epsilon_greedy(env, epsilon)\n",
    "                else:\n",
    "                    action = self._select_action_by_critic(env, self.opp_model)\n",
    "\n",
    "                reward_fn = ShapedReward(my_color)\n",
    "                next_state, reward, done, _ = env.step(action, reward_fn=reward_fn)\n",
    "                next_player = env.player\n",
    "                next_legal_actions = self._legal_actions_from_board(next_state[0], next_player)\n",
    "\n",
    "                if player == my_color:\n",
    "                    # 状態遷移を保存\n",
    "                    # 終局の場合は３回状態遷移を保存（通常時は1回）\n",
    "                    if done:\n",
    "                        num_save = 3\n",
    "                    else:\n",
    "                        num_save = 1\n",
    "                    for _ in range(num_save):\n",
    "                        self._store_transition(\n",
    "                            board=state[0],\n",
    "                            action=action,\n",
    "                            reward=reward,\n",
    "                            next_board=next_state[0],\n",
    "                            done=done,\n",
    "                            player=player,\n",
    "                            next_player=next_player,\n",
    "                            next_legal_actions=next_legal_actions,\n",
    "                        )\n",
    "                    self._num_env_steps += 1\n",
    "\n",
    "                if (\n",
    "                    (self._num_env_steps % self.train_freq == 0)\n",
    "                    and (len(self.replay_buffer) >= max(self.batch_size, self.learning_starts))\n",
    "                ):\n",
    "                    for _ in range(self.gradient_steps):\n",
    "                        loss = self._update_dqn_double()\n",
    "                        if not math.isnan(loss):\n",
    "                            self.losses.append(loss)\n",
    "\n",
    "                self._maybe_update_target()\n",
    "\n",
    "                total_reward += float(reward)\n",
    "\n",
    "            games += 1\n",
    "\n",
    "        return float(total_reward)\n",
    "\n",
    "    def _select_action_by_critic(\n",
    "        self,\n",
    "        env: OthelloEnv,\n",
    "        critic: nn.Module,\n",
    "    ) -> int:\n",
    "        \"\"\"Critic の評価値に基づき相手の行動を決定\"\"\"\n",
    "        legal_actions = env.legal_actions()\n",
    "        values = []\n",
    "\n",
    "        # 現在局面をコピーして各手をシミュレート\n",
    "        for action in legal_actions:\n",
    "            sim_game = env.game.clone()\n",
    "            sim_env = OthelloEnv()\n",
    "            sim_env.game = sim_game\n",
    "            sim_env.player = env.player\n",
    "\n",
    "            # アクション適用\n",
    "            try:\n",
    "                sim_env.step(action)\n",
    "            except Exception:\n",
    "                values.append(-float(\"inf\"))\n",
    "                continue\n",
    "\n",
    "            sim_state = sim_env.get_state()\n",
    "            input_tensor = torch.as_tensor(sim_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "            # 現在の盤面での相手の視点の評価値を計算\n",
    "            with torch.no_grad():\n",
    "                v = critic(input_tensor).item()\n",
    "\n",
    "            # Critic は現在手番視点の値を返すと仮定し、そのまま最大化\n",
    "            values.append(v)\n",
    "\n",
    "        values = np.array(values, dtype=np.float32)\n",
    "\n",
    "        best_idx = int(values.argmax())\n",
    "        return int(legal_actions[best_idx])\n",
    "\n",
    "\n",
    "    def _update_dqn_double(self) -> float:\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # (B,2,8,8)\n",
    "        board = torch.stack([self._to_input(b['board'], b['player']) for b in batch]).to(self.device)\n",
    "        next_board = torch.stack([self._to_input(b['next_board'], b['next_player']) for b in batch]).to(self.device)\n",
    "\n",
    "        action = torch.tensor([b['action'] for b in batch], dtype=torch.int64, device=self.device)\n",
    "        reward = torch.tensor([b['reward'] for b in batch], dtype=torch.float32, device=self.device)\n",
    "        done = torch.tensor([b['done'] for b in batch], dtype=torch.float32, device=self.device)\n",
    "        next_legal_actions_list = [b['next_legal_actions'] for b in batch]\n",
    "\n",
    "        # Q(s,a)\n",
    "        q_all = self.dqn(board)                                 # (B,65)\n",
    "        q_sa = q_all.gather(1, action.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 非合法手マスク\n",
    "            next_masks = self._build_masks_from_indices(next_legal_actions_list, fill_value=-1e9)\n",
    "            q_next_online = self.dqn(next_board) + next_masks\n",
    "            next_actions_online = q_next_online.argmax(dim=1)  # (B,)\n",
    "\n",
    "            q_next_target = self.target_dqn(next_board)\n",
    "            next_q = q_next_target.gather(1, next_actions_online.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            target = reward + self.gamma * next_q * (1.0 - done)\n",
    "\n",
    "        loss = self.loss_fn(q_sa, target)\n",
    "        if torch.isnan(loss):\n",
    "            return float(\"nan\")\n",
    "\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dqn.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self._num_updates += 1\n",
    "        return float(loss.item())\n",
    "\n",
    "    def _select_action_by_epsilon_greedy(self, env: OthelloEnv, epsilon: float) -> int:\n",
    "        \"\"\" ε-greedy で行動を選択する \"\"\"\n",
    "        legal_actions = env.legal_actions()\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(legal_actions)\n",
    "        return self._select_action_by_greedy(env)\n",
    "\n",
    "    def _select_action_by_greedy(self, env: OthelloEnv) -> int:\n",
    "        \"\"\" greedy で行動を選択する \"\"\"\n",
    "        legal_actions = env.legal_actions()\n",
    "        board_tensor = self._to_input(env.get_state(), env.player).unsqueeze(0).to(self.device)  # (1,2,8,8)\n",
    "        with torch.no_grad():\n",
    "            q_all = self.dqn(board_tensor).squeeze(0)  # (65,)\n",
    "            mask = torch.full((65,), -1e9, device=self.device)\n",
    "            for a in legal_actions:\n",
    "                mask[a] = 0.0\n",
    "            q_masked = q_all + mask\n",
    "            action = int(q_masked.argmax().item())\n",
    "        return action\n",
    "\n",
    "    def _init_replay_buffer(self):\n",
    "        \"\"\"\n",
    "        リプレイバッファを初期化する。\n",
    "        ターミナル遷移と非ターミナル遷移が、それぞれの目標数に達するまでゲームをプレイし続ける。\n",
    "        \"\"\"\n",
    "        target = min(self.init_memory_size, self.replay_buffer.memory_size)\n",
    "        terminal_quota = target // 3\n",
    "        non_terminal_quota = target - terminal_quota\n",
    "\n",
    "        num_terminal = 0\n",
    "        num_non_terminal = 0\n",
    "\n",
    "        pbar = tqdm(total=target, desc='Init replay buffer')\n",
    "\n",
    "        # ターミナルと非ターミナルの両方の枠が埋まるまでループ\n",
    "        while num_terminal < terminal_quota or num_non_terminal < non_terminal_quota:\n",
    "            env = OthelloEnv()\n",
    "            done = False\n",
    "            episode = []\n",
    "\n",
    "            # 1局を終局までプレイ\n",
    "            while not done:\n",
    "                player = env.player\n",
    "                state = env.get_state()\n",
    "                action = random.choice(env.legal_actions())\n",
    "                reward_fn = ShapedReward(player)\n",
    "                next_state, reward, done, _ = env.step(action, reward_fn=reward_fn)\n",
    "                next_player = env.player\n",
    "                next_legal_actions = self._legal_actions_from_board(next_state[0], next_player)\n",
    "\n",
    "                episode.append({\n",
    "                    'board': state[0], 'action': action, 'reward': reward,\n",
    "                    'next_board': next_state[0], 'done': done, 'player': player,\n",
    "                    'next_player': next_player, 'next_legal_actions': next_legal_actions,\n",
    "                })\n",
    "\n",
    "            # ターミナル遷移を追加（最後の遷移）\n",
    "            if num_terminal < terminal_quota and episode:\n",
    "                t = episode[-1]\n",
    "                self._store_transition(\n",
    "                    board=t['board'], action=t['action'], reward=t['reward'],\n",
    "                    next_board=t['next_board'], done=t['done'],\n",
    "                    player=t['player'], next_player=t['next_player'],\n",
    "                    next_legal_actions=t['next_legal_actions'],\n",
    "                )\n",
    "                num_terminal += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"terminal\": num_terminal, \"non_terminal\": num_non_terminal})\n",
    "\n",
    "            # 非ターミナル遷移を追加（最後の遷移以外）\n",
    "            for t in episode[:-1]:\n",
    "                if num_non_terminal >= non_terminal_quota:\n",
    "                    break\n",
    "                self._store_transition(\n",
    "                    board=t['board'], action=t['action'], reward=t['reward'],\n",
    "                    next_board=t['next_board'], done=False,  # 非ターミナルとして扱う\n",
    "                    player=t['player'], next_player=t['next_player'],\n",
    "                    next_legal_actions=t['next_legal_actions'],\n",
    "                )\n",
    "                num_non_terminal += 1\n",
    "\n",
    "                pbar.set_postfix({\"terminal\": num_terminal, \"non_terminal\": num_non_terminal})\n",
    "                pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    def _maybe_update_target(self):\n",
    "        \"\"\" ターゲットネットワークを更新する \"\"\"\n",
    "        if self.tau and self.tau > 0.0:\n",
    "            with torch.no_grad():\n",
    "                for tp, p in zip(self.target_dqn.parameters(), self.dqn.parameters()):\n",
    "                    tp.data.mul_(1.0 - self.tau).add_(self.tau * p.data)\n",
    "        else:\n",
    "            if (self._num_updates % max(1, self.target_update_freq)) == 0 and self._num_updates > 0:\n",
    "                self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _epsilon_by_step(self, step: int) -> float:\n",
    "        \"\"\" epsilon-greedy 用の ε を計算する \"\"\"\n",
    "        if step >= self.epsilon_decay_steps:\n",
    "            return self.epsilon_end\n",
    "        span = self.epsilon_start - self.epsilon_end\n",
    "        return self.epsilon_start - span * (step / self.epsilon_decay_steps)\n",
    "\n",
    "    def _build_masks_from_indices(self, batch_next_legal_actions: List[List[int]], fill_value: float = -1e9):\n",
    "        \"\"\" バッチの各状態の合法手をマスクする \"\"\"\n",
    "        B = len(batch_next_legal_actions)\n",
    "        masks = torch.full((B, 65), float(fill_value), device=self.device)\n",
    "        for i, acts in enumerate(batch_next_legal_actions):\n",
    "            for a in acts:\n",
    "                masks[i, a] = 0.0\n",
    "        return masks\n",
    "\n",
    "    def _to_input(self, board_like, player_scalar: int) -> torch.Tensor:\n",
    "        \"\"\" ボードを入力形式に変換する \"\"\"\n",
    "        t = torch.as_tensor(board_like, dtype=torch.float32)\n",
    "        if t.dim() == 3 and t.shape == (2, 8, 8):\n",
    "            t = t[0:1]\n",
    "        elif t.dim() == 2 and t.shape == (8, 8):\n",
    "            t = t.unsqueeze(0)\n",
    "        elif t.dim() == 3 and t.shape == (1, 8, 8):\n",
    "            pass\n",
    "        else:\n",
    "            t = t.reshape(1, 8, 8)\n",
    "        player_plane = torch.full_like(t, float(player_scalar))\n",
    "        return torch.cat([t, player_plane], dim=0)\n",
    "\n",
    "    def _legal_actions_from_board(self, board_np: np.ndarray, player: int) -> List[int]:\n",
    "                # OthelloGame を新規に作成し、numpy 配列を設定\n",
    "        g = OthelloGame()\n",
    "        # board_np は float32（-1,0,1）なので int8 に変換\n",
    "        if board_np.ndim == 3 and board_np.shape == (2, 8, 8):\n",
    "            board_np = board_np[0]\n",
    "        g.board = np.asarray(board_np, dtype=np.int8)\n",
    "        g.player = int(player)\n",
    "        moves = g.legal_moves(g.player)\n",
    "        return [64] if not moves else [r * 8 + c for r, c in moves]\n",
    "\n",
    "    def _store_transition(\n",
    "        self,\n",
    "        board,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_board,\n",
    "        done: bool,\n",
    "        player: int,\n",
    "        next_player: int,\n",
    "        next_legal_actions: List[int],\n",
    "    ):\n",
    "        \"\"\" 状態遷移を保存する \"\"\"\n",
    "        legal_actions = self._legal_actions_from_board(np.array(board, dtype=np.float32), int(player))\n",
    "        transition = {\n",
    "            \"board\": np.array(board, dtype=np.float32),\n",
    "            \"action\": int(action),\n",
    "            \"reward\": float(reward),\n",
    "            \"next_board\": np.array(next_board, dtype=np.float32),\n",
    "            \"done\": bool(done),\n",
    "            \"player\": int(player),\n",
    "            \"next_player\": int(next_player),\n",
    "            \"legal_actions\": legal_actions,\n",
    "            \"next_legal_actions\": next_legal_actions,\n",
    "        }\n",
    "        self.replay_buffer.append(transition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef70493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db504d782a045eda1d85063f41aec3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_out = widgets.Output()\n",
    "loss_out = widgets.Output()\n",
    "opponent_out = widgets.Output()\n",
    "display(widgets.VBox([reward_out, loss_out, opponent_out]))\n",
    "\n",
    "rewards_history = []\n",
    "losses_history = []\n",
    "opponent_histories = {}\n",
    "\n",
    "def live_callback(ep, reward, rolling_avg, latest_loss, info=None):\n",
    "    rewards_history.append(reward)\n",
    "    if not math.isnan(latest_loss):\n",
    "        losses_history.append(latest_loss)\n",
    "\n",
    "    # 全体報酬プロット\n",
    "    with reward_out:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(6, 3))\n",
    "        ax.plot(rewards_history, label=\"Episode Reward\")\n",
    "        # 0の基準線\n",
    "        ax.axhline(0.0, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.8)\n",
    "        if len(rewards_history) >= 20:\n",
    "            rolling = np.convolve(rewards_history, np.ones(20) / 20, mode=\"valid\")\n",
    "            ax.plot(range(19, 19 + len(rolling)), rolling, label=\"MA@20\", color=\"tab:orange\")\n",
    "        ax.set_title(f\"Episode {ep + 1}\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # 損失プロット\n",
    "    with loss_out:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(6, 3))\n",
    "        if losses_history:\n",
    "            ax.plot(losses_history, label=\"Training Loss\", color=\"tab:red\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # 対戦モードごとの報酬プロット\n",
    "    if info is not None:\n",
    "        opponent_histories.clear()\n",
    "        for mode, hist in info.get(\"history\", {}).items():\n",
    "            opponent_histories[mode] = list(hist)\n",
    "\n",
    "        with opponent_out:\n",
    "            clear_output(wait=True)\n",
    "            fig, ax = plt.subplots(figsize=(6, 3))\n",
    "            for mode, hist in opponent_histories.items():\n",
    "                if hist:\n",
    "                    ax.plot(hist, label=f\"{mode} vs reward\")\n",
    "            # 0の基準線\n",
    "            ax.axhline(0.0, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.8)\n",
    "            ax.set_title(f\"Latest opponent: {info.get('opponent', 'unknown')}\")\n",
    "            ax.grid(True)\n",
    "            ax.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52fdaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae02f6a5e1864db3bed116ce732b23b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Init replay buffer:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceeeb0d98a14e2fb7ff4772aba86c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Double DQN:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = \"models\"\n",
    "progress_callback = live_callback\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "dqn = DQN().to(device)\n",
    "trainer = TrainDoubleDQN(\n",
    "    dqn=dqn,\n",
    "    device=device,\n",
    "    ReplayBufferCls=ReplayBuffer,\n",
    "    num_episodes=2e3,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    lr=5e-4,\n",
    "    target_update_freq=500,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay_steps=1e5,\n",
    "    max_games_per_episode=1,\n",
    "    init_memory_size=5e3,\n",
    "    memory_size=1e4,\n",
    "    learning_starts=1e3,\n",
    "    rolling_window=20,\n",
    "    save_best_path=os.path.join(save_dir, \"best.pth\"),\n",
    "    pretrained_opp_path = os.path.join(save_dir, \"critic\", \"pretrained_criticnet.pt\"),\n",
    "    progress_callback=live_callback,\n",
    ")\n",
    "\n",
    "metrics, best_model = trainer.train(return_best_model=True)\n",
    "\n",
    "# 保存\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_path = os.path.join(save_dir, f\"best_{ts}.pth\")\n",
    "torch.save(best_model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Best model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
